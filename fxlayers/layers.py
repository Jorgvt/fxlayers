# AUTOGENERATED! DO NOT EDIT! File to edit: ../Notebooks/00_layers.ipynb.

# %% auto 0
__all__ = ['GaussianLayer', 'GaborLayer', 'CenterSurroundLogSigma', 'CenterSurroundLogSigmaK', 'GaborLayer_', 'JamesonHurvich',
           'CSFFourier', 'GDN', 'GDNStar', 'GDNStarSign', 'GDNDisplacement', 'GDNStarDisplacement', 'GDNStarRunning',
           'GDNStarDisplacementRunning', 'FreqGaussian', 'OrientGaussian']

# %% ../Notebooks/00_layers.ipynb 4
import jax
from typing import Any, Callable, Sequence, Union
from jax import lax, random, numpy as jnp
from flax.core import freeze, unfreeze
from flax import linen as nn
import optax
from einops import rearrange, repeat

from .initializers import *

# %% ../Notebooks/00_layers.ipynb 8
class GaussianLayer(nn.Module):
    """Parametric gaussian layer."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        sigma = self.param("sigma",
                           nn.initializers.uniform(scale=self.xmean),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.

        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True, normalize_energy=False):
        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
        gaussian = A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(gaussian**2)), 1.)
        return A*gaussian/E_norm

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, params["params"]["sigma"], params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])

# %% ../Notebooks/00_layers.ipynb 9
class GaussianLayerLogSigma(nn.Module):
    """Parametric gaussian layer that optimizes log(sigma) instead of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        logsigma = self.param("logsigma",
                           bounded_uniform(minval=-4., maxval=-0.5),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigma = jnp.exp(logsigma)
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True, normalize_energy=False):
        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
        g = A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["logsigma"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])

# %% ../Notebooks/00_layers.ipynb 21
class GaborLayer(nn.Module):
    """Parametric Gabor layer."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency

    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        freq = self.param("freq",
                           nn.initializers.uniform(scale=self.fs/2),
                           (self.features*inputs.shape[-1],))
        logsigmax = self.param("logsigmax",
                           bounded_uniform(minval=-4., maxval=-0.5),
                           (self.features*inputs.shape[-1],))
        logsigmay = self.param("logsigmay",
                           bounded_uniform(minval=-4., maxval=-0.5),
                           (self.features*inputs.shape[-1],))        
        theta = self.param("theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features*inputs.shape[-1],))
        sigma_theta = self.param("sigma_theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features*inputs.shape[-1],))
        rot_theta = self.param("rot_theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigmax, sigmay = jnp.exp(logsigmax), jnp.exp(logsigmay)
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            # gabor_fn = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,None,None))
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, A, self.normalize_prob, self.normalize_energy)
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, A=1, normalize_prob=True, normalize_energy=False):
        # ## Rotate the dominion
        # x = jnp.cos(rot_theta) * (x - xmean) - jnp.sin(rot_theta) * (y - ymean)
        # y = jnp.sin(rot_theta) * (x - xmean) + jnp.cos(rot_theta) * (y - ymean)
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        sigma_vector = jnp.array([sigmax, sigmay])
        cov_matrix = jnp.diag(sigma_vector)**2
        det_cov_matrix = jnp.linalg.det(cov_matrix)
        # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigmax, sigmay = jnp.exp(params["logsigmax"]), jnp.exp(params["logsigmay"])
        # sigmax, sigmay = jnp.exp(params["sigmax"]), jnp.exp(params["sigmay"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, params["freq"], params["theta"], params["sigma_theta"], params["rot_theta"], params["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, input_channels, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])

# %% ../Notebooks/00_layers.ipynb 34
class CenterSurroundLogSigma(nn.Module):
    """Parametric center surround layer that optimizes log(sigma) instead of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        logsigma = self.param("logsigma",
                           bounded_uniform(minval=-2.2, maxval=-1.7),
                           (self.features*inputs.shape[-1],))
        logsigma2 = self.param("logsigma2",
                           bounded_uniform(minval=-2.2, maxval=-1.7),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigma = jnp.exp(logsigma)
        sigma2 = jnp.exp(logsigma2)
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, sigma2, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    # @staticmethod
    # def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
    #     # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
    #     A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma), 1.)
    #     return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
    
    @staticmethod
    def center_surround(x, y, xmean, ymean, sigma, sigma2, A=1, normalize_prob=True, normalize_energy=False):
        def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
            A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
            return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        g1 = gaussian(x, y, xmean, ymean, sigma, 1, normalize_prob)
        g2 = gaussian(x, y, xmean, ymean, sigma2, 1, normalize_prob)
        g = g1-g2
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm
    
    # @staticmethod
    # def center_surround(x, y, xmean, ymean, sigma,  K, A=1, normalize_prob=True):
    #     return (1/(2*jnp.pi*sigma**2))*(jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2)) - (1/(K**2))*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*(K*sigma)**2)))

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["logsigma"]), jnp.exp(params["params"]["logsigma2"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])

# %% ../Notebooks/00_layers.ipynb 37
class CenterSurroundLogSigmaK(nn.Module):
    """Parametric center surround layer that optimizes log(sigma) instead of sigma and has a factor K instead of a second sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = True

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        logsigma = self.param("logsigma",
                           bounded_uniform(minval=-2.2, maxval=-1.7),
                           (self.features*inputs.shape[-1],))
        K = self.param("K",
                           displaced_normal(mean=1.1, stddev=0.1),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigma = jnp.exp(logsigma)
        sigma2 = K*sigma
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, sigma2, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    # @staticmethod
    # def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
    #     # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
    #     A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma), 1.)
    #     return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
    
    @staticmethod
    def center_surround(x, y, xmean, ymean, sigma, sigma2, A=1, normalize_prob=True, normalize_energy=False):
        def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
            A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
            return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        g1 = gaussian(x, y, xmean, ymean, sigma, 1, normalize_prob)
        g2 = gaussian(x, y, xmean, ymean, sigma2, 1, normalize_prob)
        g = g1 - g2
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm
    
    # @staticmethod
    # def center_surround(x, y, xmean, ymean, sigma,  K, A=1, normalize_prob=True):
    #     return (1/(2*jnp.pi*sigma**2))*(jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2)) - (1/(K**2))*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*(K*sigma)**2)))

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["logsigma"]), params["params"]["K"]*jnp.exp(params["params"]["logsigma"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])

# %% ../Notebooks/00_layers.ipynb 49
class GaborLayer_(nn.Module):
    """Parametric Gabor layer with particular initialization."""
    # features: int
    n_scales: int
    n_orientations: int
    # n_phases: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    phase = jnp.array([0., jnp.pi/2.])

    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        features = self.n_scales * self.n_orientations * len(self.phase)
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                           freq_scales_init(n_scales=self.n_scales, fs=self.fs),
                           (self.n_scales,))
        sigmax = self.param("sigmax",
                           k_array(k=0.4, arr=freq),
                           (self.n_scales,))
        sigmay = self.param("sigmay",
                           equal_to(1.5*sigmax),
                           (self.n_scales,))
        # theta = self.param("theta",
        #                    nn.initializers.uniform(scale=jnp.pi),
        #                    (self.n_scales*self.n_orientations,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        sigma_theta = self.param("sigma_theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        # sigma_theta = theta - jnp.pi/2.
        # A = self.param("A",
        #                nn.initializers.ones,
        #                (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None), out_axes=0)
            # kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,0,None,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, freq, theta, sigma_theta, self.phase, 1, self.normalize_prob, self.normalize_energy)
            kernel = rearrange(kernel, "phases rots fs_sigmas kx ky -> kx ky (phases rots fs_sigmas)")
            kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=kernel.shape[-1])
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax, sigmay, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        sigma_vector = jnp.array([sigmax, sigmay])
        cov_matrix = jnp.diag(sigma_vector)**2
        det_cov_matrix = jnp.linalg.det(cov_matrix)
        # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigmax, sigmay = jnp.exp(params["sigmax"]), jnp.exp(params["sigmay"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,None,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,0,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, params["sigmax"], params["sigmay"], params["freq"], params["theta"], params["sigma_theta"], self.phase, 1, self.normalize_prob, self.normalize_energy)
        # kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
        kernel = rearrange(kernel, "rots fs sigmas kx ky -> kx ky (rots fs sigmas)")
        kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=c_in, c_out=kernel.shape[-1])
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])

# %% ../Notebooks/00_layers.ipynb 60
class JamesonHurvich(nn.Module):
    """Jameson & Hurvich transformation from RGB to ATD."""

    def setup(self):
        self.Mng2xyz = jnp.array([[69.1661, 52.4902, 46.6052],
                                  [39.0454, 115.8404, 16.3118],
                                  [3.3467, 12.6700, 170.1090]])
        self.Mxyz2atd = jnp.array([[0, 1, 0],
                                   [1, -1, 0],
                                   [0, 0.4, -0.4]])

    def __call__(self,
                 inputs, # (B,H,W,C)
                 **kwargs,
                 ):
        outputs = inputs**2
        outputs = inputs @ self.Mng2xyz.T @ self.Mxyz2atd.T
        return outputs

# %% ../Notebooks/00_layers.ipynb 64
def metefot(sec, foto, N, ma):
    ss = foto.shape
    fil = ss[0]
    col = ss[1]
    s = sec.shape
    Nfot = s[1] / col

    if N > Nfot:
        sec = [sec, foto]
    else:
        if ma == 1:
            sec = sec.at[:, (N-1)*col:N*col].set(foto)
    # if incorrect results finish this function.
    return sec

# %% ../Notebooks/00_layers.ipynb 65
def freqspace(N):
    # Returns 2-d frequency range vectors for N[0] x N[1] matrix

    f1 = (jnp.arange(0, N[0], 1)-jnp.floor(N[0]/2))*(2/N[0])
    f2 = (jnp.arange(0, N[1], 1)-jnp.floor(N[1]/2))*(2/N[1])
    F1, F2 = jnp.meshgrid(f1, f2)
    return F1, F2

# %% ../Notebooks/00_layers.ipynb 66
def spatio_temp_freq_domain(Ny, Nx, Nt, fsx, fsy, fst):
    int_x = Nx/fsx # Physical domain
    int_y = Ny/fsy
    int_t = Nt/fst

    x = jnp.zeros((Ny, Nx*Nt)) # Big matrix
    y = jnp.zeros((Ny, Nx*Nt))
    t = jnp.zeros((Ny, Nx*Nt))

    fot_x = jnp.linspace(0, int_x, Nx+1)
    fot_x = fot_x[:-1]
    fot_x = fot_x[None,:].repeat(Ny, 0)

    fot_y = jnp.linspace(0, int_y, Ny+1)
    fot_y = fot_y[:-1]
    fot_y = fot_y[:,None].repeat(Nx, 1)

    fot_t = jnp.ones((Ny, Nx))

    val_t = jnp.linspace(0, int_t, Nt+1)
    val_t = val_t[:-1]

    for i in range(Nt):
        x = metefot(x, fot_x, i+1, 1)
        y = metefot(y, fot_y, i+1, 1)
        t = metefot(t, val_t[i]*fot_t, i+1, 1)

    [fx, fy] = freqspace([Nx, Ny])

    fx = fx*fsx/2
    fy = fy*fsy/2

    ffx = jnp.zeros((Ny, Nx*Nt))
    ffy = jnp.zeros((Ny, Nx*Nt))
    ff_t = jnp.zeros((Ny, Nx*Nt))

    fot_fx = fx
    fot_fy = fy
    fot_t = jnp.ones((Ny, Nx))

    [ft, ft2] = freqspace([Nt, Nt])
    val_t = ft*fst/2

    for i in range(Nt):
        ffx = metefot(ffx, fot_fx, i+1, 1)
        ffy = metefot(ffy, fot_fy, i+1, 1)
        ff_t = metefot(ff_t, val_t[0,:][i]*fot_t, i+1, 1)

    return x, y, t, ffx, ffy, ff_t

# %% ../Notebooks/00_layers.ipynb 67
class CSFFourier(nn.Module):
    """CSF SSO."""
    fs: int = 64
    norm_energy: bool = True

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        alpha_achrom = self.param("alpha_achrom",
                                  equal_to(1.),
                                  (1,))
        alpha_chrom_rg = self.param("alpha_chrom_rg",
                                 equal_to(1.),
                                 (1,))
        alpha_chrom_yb = self.param("alpha_chrom_yb",
                                 equal_to(1.),
                                 (1,))
        beta_achrom = self.param("beta_achrom",
                          equal_to(1.),
                          (1,))
        beta_chrom = self.param("beta_chrom",
                          equal_to(1.),
                          (1,))
        fm = self.param("fm",
                        equal_to(7.28),
                        (1,))
        s = self.param("s",
                       equal_to(1.809),
                       (1,))
        
        b, h, w, c = inputs.shape

        ## 1. Achromatic CSF
        # csf, fx, fy = jax.jit(self.csf_sso, static_argnums=(1,2))(fs=self.fs, Nx=w, Ny=h, alpha=alpha_achrom, beta=beta_achrom, g=330.74, fm=fm, l=0.837, s=s, w=1.0, os=6.664)
        csf, fx, fy = self.csf_sso(fs=self.fs, Nx=w, Ny=h, alpha=alpha_achrom, beta=beta_achrom, g=330.74, fm=fm, l=0.837, s=s, w=1.0, os=6.664)

        # jax.debug.print(f"Nx={w}, Ny={h}, {csf.shape}")
        ## 2. Chromatic CSFs
        # csfrg, csfyb, fx, fy = jax.jit(self.csf_chrom, static_argnums=(1,2))(fs=self.fs, Nx=w, Ny=h, alpha_rg=alpha_chrom_rg, alpha_yb=alpha_chrom_yb, beta=beta_chrom)
        csfrg, csfyb, fx, fy = self.csf_chrom(fs=self.fs, Nx=w, Ny=h, alpha_rg=alpha_chrom_rg, alpha_yb=alpha_chrom_yb, beta=beta_chrom)
        # jax.debug.print(f"Nx={w}, Ny={h}, {csf.shape}, {csfrg.shape}, {csfyb.shape}")

        ## 3. Stack the three CSFs together
        csfs = jnp.stack([csf, csfrg, csfyb], axis=-1)

        ## 4. FFT of the input
        inputs_fft = jnp.fft.fft2(inputs, axes=(1,2))
        inputs_fft = jnp.fft.fftshift(inputs_fft)
        
        ## 5. Apply the CSF by multiplying
        E1 = jnp.sum(jnp.ones_like(csfs)**2)#**(1/2)
        E_CSF = jnp.sum(csfs**2)#**(1/2)
        if self.norm_energy: csfs = (csfs/E_CSF)*E1
        inputs_fft = csfs[None,:]*inputs_fft

        ## 6. Return to the original domain
        outputs = jnp.fft.ifft2(jnp.fft.ifftshift(inputs_fft), axes=(1,2))
        outputs = jnp.real(outputs)
        
        return outputs
    
    @staticmethod
    def csf_sso(fs, Nx, Ny, alpha, beta, g=330.74, fm=7.28, l=0.837, s=1.809, w=1.0, os=6.664):

        [_,_,_,fx,fy,_] = spatio_temp_freq_domain(Nx=Nx, Ny=Ny, Nt=1, fsx=fs, fsy=fs, fst=1)
        fx, fy = fx*beta, fy*beta
        f = jnp.sqrt(jnp.clip(fx**2 + fy**2, a_min=0.0001))
        # f = f.at[f == 0].set(0.0001)

        CSFT = g * (jnp.exp(-(f/fm)) - l*jnp.exp(-(f**2/s**2)))
        OE = 1 - w*(4*(1-jnp.exp(-(f/os)))*fx**2 *fy**2)/(f**4)
        CSFSSO = alpha*(CSFT * OE)

        return CSFSSO, fx, fy
    
    @staticmethod
    def csf_chrom(fs, Nx, Ny, alpha_rg, alpha_yb, beta):

        def sigm1d(x,x0,s):
            y = 1/(1+jnp.exp((x-x0)/s))
            return y
        
        def umbinc3(c,cu,k,m,alf,sig):
            umb = (cu-k*cu**m)*(1/(1+jnp.exp(jnp.log10(c/(alf*cu))/sig)))+(k*c**m)*(1-1/(1+jnp.exp(jnp.log10(c/(0.9*cu))/(sig/2))))
            return umb
        
        # def iafrg(f, C, facfrec, nolin):
        #     f = facfrec*f
        #     f = jnp.clip(f, a_min=0.00001)
        #     C = jnp.clip(C, a_min=0.0000001)

        #     lf = len(f)
        #     lc = len(C)

        #     #iaf = np.zeros(lf,lc)
        #     ace = jnp.zeros((lf,lc))
        #     p = [0.0840, 0.8345, 0.6313, 0.2077]

        #     if len(nolin)==1:
        #         nolin = [nolin, nolin]

        #     nolini = nolin
        #     nolin = nolini[0]

        #     if ((nolini[0]==0)&(nolini[1]==1)):
        #         nolin=1


        #     if nolin==1:
        #         for i in range(lf):
        #             cu = 1/(100*2537.9*sigm1d(f[i],-55.94,6.64))
        #             ace[i,:] = umbinc3(C,cu,p[0],p[1],p[2],p[3])

        #         iaf = 1/ace

        #     else:
        #         iaf=100*2537.9*sigm1d(f,-55.94,6.64)
        #         iaf=iaf*jnp.ones((1,len(C)))

        #     csfrg=iaf[0,:]

        #     if ((nolini[0]==0)&(nolini[1]==1)):
        #         s = iaf.shape
        #         iafc = jnp.sum(iaf)
        #         iaf = iafc*jnp.ones((1,s[1]))

        #     return iaf, csfrg
        
        def iafrg(f, C, facfrec, nolin, lenf, lenC):
            f = facfrec*f
            f = jnp.clip(f, a_min=0.00001)
            C = jnp.clip(C, a_min=0.0000001)

            # lf = len(f)
            # lc = len(C)
            lf, lc = lenf, lenC

            #iaf = np.zeros(lf,lc)
            ace = jnp.zeros((lf,lc))
            p = [0.0840, 0.8345, 0.6313, 0.2077]

            # if len(nolin)==1:
            #     nolin = [nolin, nolin]

            nolini = nolin
            nolin = nolini[0]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            #     nolin=1


            # if nolin==1:
            #     for i in range(lf):
            #         cu = 1/(100*2537.9*sigm1d(f[i],-55.94,6.64))
            #         ace[i,:] = umbinc3(C,cu,p[0],p[1],p[2],p[3])

            #     iaf = 1./ace

            # else:
            iaf=100*2537.9*sigm1d(f,-55.94,6.64)
            iaf=iaf*jnp.ones((1,len(C)))

            csfrg=iaf[0,:]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            s = iaf.shape
            iafc = jnp.sum(iaf)
            iaf = iafc*jnp.ones((1,s[1]))

            return iaf, csfrg

        def iafyb(f, C, facfrec, nolin, lenf, lenC):
            f = facfrec*f
            f = jnp.clip(f, a_min=0.00001)
            C = jnp.clip(C, a_min=0.0000001)

            # lf = len(f)
            # lc = len(C)
            lf, lc = lenf, lenC

            #iaf = np.zeros(lf,lc)
            ace = jnp.zeros((lf,lc))
            p = [0.1611, 1.3354, 0.3077, 0.7746]

            # if len(nolin)==1:
            #     nolin = [nolin, nolin]

            nolini = nolin
            nolin = nolini[0]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            #     nolin=1

            # if nolin==1:
            #     for i in range(lf):
            #         cu=1/(100*719.7*sigm1d(f[i],-31.72,4.13))
            #         ace[i,:]=umbinc3(C,cu,p[0],p[1],p[2],p[3])

            #     iaf = 1/ace

            # else:
            iaf=100*719.7*sigm1d(f,-31.72,4.13)
            iaf=iaf*jnp.ones((1,len(C)))

            csfyb=iaf[0,:]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            s=iaf.shape
            iafc=jnp.sum(iaf)
            iaf=iafc*jnp.ones((1,s[1]))
            
            return iaf, csfyb

        [_,_,_,fx,fy,_] = spatio_temp_freq_domain(Nx=Nx, Ny=Ny, Nt=1, fsx=fs, fsy=fs, fst=1)
        fx, fy = fx*beta, fy*beta
        # f = jnp.sqrt(fx**2 + fy**2)
        f = jnp.sqrt(jnp.clip(fx**2 + fy**2, a_min=0.0001))
        #f[f == 0] = 0.0001

        csfrg = jnp.zeros((Ny,Nx))
        csfyb = jnp.zeros((Ny,Nx))

        for i in range(Ny):
            [iaf_rg, csf_c] = iafrg(f[i,:], jnp.array([0.1]), 1., jnp.array([0., 0., 0.]), len(f[i,:]), len(jnp.array([0.1])))
            # jax.debug.print(f"{csfrg.shape} / {csf_c.shape}")
            csfrg = csfrg.at[i,:].set(csf_c)

            [iaf_yb, csf_c] = iafyb(f[i,:], jnp.array([0.1]), 1., jnp.array([0., 0., 0.]), len(f[i,:]), len(jnp.array([0.1])))
            csfyb = csfyb.at[i,:].set(csf_c)

        fact_rg = 0.75
        fact_yb = 0.55
        max_CSF_achro = 201.3

        csfrg = fact_rg*max_CSF_achro*csfrg/jnp.max(csfrg)
        csfyb = fact_yb*max_CSF_achro*csfyb/jnp.max(csfyb)

        return alpha_rg*csfrg, alpha_yb*csfyb, fx, fy

# %% ../Notebooks/00_layers.ipynb 90
class GDN(nn.Module):
    """Generalized Divisive Normalization."""
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    apply_independently: bool = False
    # kernel_init: Callable = nn.initializers.lecun_normal()
    kernel_init: Callable = mean()
    bias_init: Callable = nn.initializers.ones_init()
    alpha: float = 2.
    epsilon: float = 1/2 # Exponential of the denominator
    eps: float = 1e-6 # Numerical stability in the denominator

    @nn.compact
    def __call__(self,
                 inputs,
                 ):
        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input
                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, 
                        strides=self.strides, 
                        padding=self.padding,
                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,
                        kernel_init=self.kernel_init, 
                        bias_init=self.bias_init)(inputs**self.alpha)
        return inputs / (jnp.clip(denom, a_min=1e-5)**self.epsilon + self.eps)

# %% ../Notebooks/00_layers.ipynb 92
class ClippedModule(nn.Module):
    layer: nn.Module
    a_min: float = -jnp.inf
    a_max: float = jnp.inf

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        return jnp.clip(self.layer(inputs, **kwargs), a_min=self.a_min, a_max=self.a_max)

# %% ../Notebooks/00_layers.ipynb 93
class GDNStar(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    kernel_init: Callable = nn.initializers.ones_init()
    bias_init: Callable = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H(inputs**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        return coef*inputs/denom

# %% ../Notebooks/00_layers.ipynb 102
class GDNStarSign(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        inputs_sign = jnp.sign(inputs)
        inputs = jnp.abs(inputs)
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H(inputs**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        return coef*inputs*inputs_sign/denom

# %% ../Notebooks/00_layers.ipynb 110
class GDNDisplacement(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        inputs_mean = inputs.mean(axis=(1,2), keepdims=True)
        inputs_mean = jnp.ones_like(inputs)*inputs_mean
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        # inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H((inputs-inputs_mean)**self.alpha), a_min=1e-5)**self.epsilon
        # coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        coef = 1.
        return coef*(inputs-inputs_mean)/denom

# %% ../Notebooks/00_layers.ipynb 114
class GDNStarDisplacement(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        inputs_mean = inputs.mean(axis=(1,2), keepdims=True)
        inputs_mean = jnp.ones_like(inputs)*inputs_mean
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H((inputs-inputs_mean)**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        # coef = 1.
        return coef*(inputs-inputs_mean)/denom

# %% ../Notebooks/00_layers.ipynb 120
class GDNStarRunning(nn.Module):
    """GDN variation where x^* is obtained as a running mean of the previously obtained values."""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    # inputs_star: float = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        is_initialized = self.has_variable("batch_stats", "inputs_star")
        # inputs_star = self.variable("batch_stats", "inputs_star", lambda x: x, jnp.quantile(inputs, q=0.95))
        inputs_star = self.variable("batch_stats", "inputs_star", jnp.ones, (1,))
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value
        denom = jnp.clip(H((inputs)**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star_**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star_
        if is_initialized and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2
        return coef*inputs/denom

# %% ../Notebooks/00_layers.ipynb 127
class GDNStarDisplacementRunning(nn.Module):
    """GDN variation where x^* is obtained as a running mean of the previously obtained values."""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    # inputs_star: float = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        is_initialized = self.has_variable("batch_stats", "inputs_star")
        # inputs_star = self.variable("batch_stats", "inputs_star", lambda x: x, jnp.quantile(inputs, q=0.95))
        inputs_star = self.variable("batch_stats", "inputs_star", jnp.ones, (1,))
        inputs_mean = inputs.mean(axis=(1,2), keepdims=True)
        inputs_mean = jnp.ones_like(inputs)*inputs_mean
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value
        denom = jnp.clip(H((inputs-inputs_mean)**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star_**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star_
        if is_initialized and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2
        return coef*(inputs-inputs_mean)/denom

# %% ../Notebooks/00_layers.ipynb 135
class FreqGaussian(nn.Module):
    """(1D) Gaussian interaction between frequencies."""
    use_bias: bool = False
    strides: int = 1
    padding: str = "SAME"
    bias_init: Callable = nn.initializers.zeros_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 fmean,
                 **kwargs,
                 ):
        sigma = self.param("sigma",
                           k_array(0.4, arr=1/fmean),
                           (inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (len(fmean),))
        else: bias = 0.
        n_groups = inputs.shape[-1] // len(fmean)
        kernel = jax.vmap(self.gaussian, in_axes=(None,0,0,None), out_axes=0)(fmean, fmean, sigma, 1)
        kernel = kernel[None,None,:,:]
        kernel = jnp.repeat(kernel, repeats=n_groups, axis=-1)

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(
                jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
                jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
                (self.strides, self.strides),
                self.padding,
                feature_group_count=n_groups)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(f, fmean, sigma, A=1):
        return A*jnp.exp(-((f-fmean)**2)/(2*sigma**2))

# %% ../Notebooks/00_layers.ipynb 143
def wrapTo180(angle, # Deg
              ):
    """Wraps an angle to the range [-180, 180]."""
    angle =  angle % 360
    angle = (angle + 360) % 360        
    return jnp.where(angle>180, angle-360, angle)

# %% ../Notebooks/00_layers.ipynb 145
def process_angles(angle1, # Deg.
                   angle2, # Deg
                   ):
    """Takes two angles as input and outputs their difference making all necessary assumptions."""
    dif = angle1 - angle2
    dif2 = dif + 180
    return jnp.min(jnp.stack([jnp.abs(wrapTo180(dif)), jnp.abs(wrapTo180(dif2))]), axis=0)

# %% ../Notebooks/00_layers.ipynb 147
class OrientGaussian(nn.Module):
    """(1D) Gaussian interaction between orientations."""
    use_bias: bool = False
    strides: int = 1
    padding: str = "SAME"
    bias_init: Callable = nn.initializers.zeros_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 theta_mean,
                 **kwargs,
                 ):
        sigma = self.param("sigma",
                        #    equal_to([jnp.pi/4]*len(theta_mean)),
                           equal_to([30]*len(theta_mean)),
                           (inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (len(fmean),))
        else: bias = 0.
        n_groups = inputs.shape[-1] // len(theta_mean)
        kernel = jax.vmap(self.gaussian, in_axes=(None,0,0,None), out_axes=0)(theta_mean, theta_mean, sigma, 1)
        kernel = kernel[None,None,:,:]
        kernel = jnp.repeat(kernel, repeats=n_groups, axis=-1)

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(
                jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
                jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
                (self.strides, self.strides),
                self.padding,
                feature_group_count=n_groups)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(theta, theta_mean, sigma, A=1):
        return A*jnp.exp(-(process_angles(theta, theta_mean)**2)/(2*sigma**2))
