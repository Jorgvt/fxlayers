# AUTOGENERATED! DO NOT EDIT! File to edit: ../Notebooks/00_layers.ipynb.

# %% auto 0
__all__ = ['GaussianLayer', 'GaborLayer', 'CenterSurroundLogSigma', 'CenterSurroundLogSigmaK', 'GaborLayer_',
           'GaborLayerGammaRepeat', 'JamesonHurvich', 'CSFFourier', 'pad_same_from_kernel_size', 'GDN', 'GDNGamma',
           'GDNGaussian', 'GDNStar', 'GDNStarSign', 'GDNDisplacement', 'GDNControl', 'GDNStarDisplacement',
           'GDNStarRunning', 'GDNStarDisplacementRunning', 'FreqGaussian', 'OrientGaussian', 'GDNGaussianStarRunning',
           'GDNSpatioFreqOrient', 'GaborGammaFourier']

# %% ../Notebooks/00_layers.ipynb 4
import jax
from typing import Any, Callable, Sequence, Union
from jax import lax, random, numpy as jnp
from flax.core import freeze, unfreeze
from flax import linen as nn
import optax
from einops import rearrange, repeat

from .initializers import *

# %% ../Notebooks/00_layers.ipynb 8
class GaussianLayer(nn.Module):
    """Parametric gaussian layer."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1]//self.feature_group_count, self.features))
        sigma = self.param("sigma",
                           nn.initializers.uniform(scale=self.xmean),
                           (self.features*inputs.shape[-1]//self.feature_group_count,))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1]//self.feature_group_count,))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.

        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1]//self.feature_group_count, c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding,
               feature_group_count=self.feature_group_count)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True, normalize_energy=False):
        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
        gaussian = A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(gaussian**2)), 1.)
        return A*gaussian/E_norm

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, params["params"]["sigma"], params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in//self.feature_group_count, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 9
class GaussianLayerLogSigma(nn.Module):
    """Parametric gaussian layer that optimizes log(sigma) instead of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1]//self.feature_group_count, self.features))
        logsigma = self.param("logsigma",
                           bounded_uniform(minval=-4., maxval=-0.5),
                           (self.features*inputs.shape[-1]//self.feature_group_count,))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1]//self.feature_group_count,))
        sigma = jnp.exp(logsigma)
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1]//self.feature_group_count, c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding,
               feature_group_count=self.feature_group_count)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True, normalize_energy=False):
        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
        g = A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["logsigma"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in//self.feature_group_count, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 10
class GaussianLayerGamma(nn.Module):
    """Parametric gaussian layer that optimizes gamma=1/sigma instead of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False
    normalize_sum: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1]//self.feature_group_count, self.features))
        gamma = self.param("gamma",
                            nn.initializers.uniform(scale=1/self.xmean),
                           (self.features*inputs.shape[-1]//self.feature_group_count,))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1]//self.feature_group_count,))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, gamma, A, self.normalize_prob, self.normalize_energy, self.normalize_sum)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1]//self.feature_group_count, c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding,
               feature_group_count=self.feature_group_count)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(x, y, xmean, ymean, gamma, A=1, normalize_prob=True, normalize_energy=False, normalize_sum=False):
        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, gamma**2/(2*jnp.pi), 1.)
        g = A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)*((gamma**2)/2))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        E_sum = jnp.where(normalize_sum, 1/g.sum(), 1.)
        return A*g*E_sum/E_norm

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["gamma"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in//self.feature_group_count, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 27
class GaborLayer(nn.Module):
    """Parametric Gabor layer."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency

    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        freq = self.param("freq",
                           nn.initializers.uniform(scale=self.fs/2),
                           (self.features*inputs.shape[-1],))
        logsigmax = self.param("logsigmax",
                           bounded_uniform(minval=-4., maxval=-0.5),
                           (self.features*inputs.shape[-1],))
        logsigmay = self.param("logsigmay",
                           bounded_uniform(minval=-4., maxval=-0.5),
                           (self.features*inputs.shape[-1],))        
        theta = self.param("theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features*inputs.shape[-1],))
        sigma_theta = self.param("sigma_theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features*inputs.shape[-1],))
        rot_theta = self.param("rot_theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigmax, sigmay = jnp.exp(logsigmax), jnp.exp(logsigmay)
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            # gabor_fn = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,None,None))
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, A, self.normalize_prob, self.normalize_energy)
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, A=1, normalize_prob=True, normalize_energy=False):
        # ## Rotate the dominion
        # x = jnp.cos(rot_theta) * (x - xmean) - jnp.sin(rot_theta) * (y - ymean)
        # y = jnp.sin(rot_theta) * (x - xmean) + jnp.cos(rot_theta) * (y - ymean)
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        sigma_vector = jnp.array([sigmax, sigmay])
        cov_matrix = jnp.diag(sigma_vector)**2
        det_cov_matrix = jnp.linalg.det(cov_matrix)
        # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)))
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigmax, sigmay = jnp.exp(params["logsigmax"]), jnp.exp(params["logsigmay"])
        # sigmax, sigmay = jnp.exp(params["sigmax"]), jnp.exp(params["sigmay"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, params["freq"], params["theta"], params["sigma_theta"], params["rot_theta"], params["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, input_channels, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 39
class GaborLayerLogSigma(nn.Module):
    """Parametric Gabor layer with particular initialization and optimizing log(sigma^2) insted of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        c_in = inputs.shape[-1]
        features = self.features
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                          nn.initializers.uniform(scale=self.fs/2),
                          (self.features*c_in,))
        logsigmax2 = self.param("logsigmax2",
                                log_k_array(k=0.3, arr=1/freq**2),
                                (self.features*c_in,))
        logsigmay2 = self.param("logsigmay2",
                                equal_to(0.8*logsigmax2),
                                (self.features*c_in,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.features*c_in),
                           (self.features*c_in,))
        sigma_theta = self.param("sigma_theta",
                           equal_to(theta),
                           (self.features*c_in,))
        sigmax2 = jnp.exp(logsigmax2)
        sigmay2 = jnp.exp(logsigmay2)
        # A = self.param("A",
        #                nn.initializers.ones,
        #                (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,None,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, freq, theta, sigma_theta, 0, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax2, sigmay2, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        cov_matrix = jnp.diag(jnp.array([sigmax2, sigmay2]))
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(jnp.linalg.det(cov_matrix))), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigma_theta = params["theta"]
        sigmax2 = jnp.exp(params["logsigmax2"])
        sigmay2 = jnp.exp(params["logsigmay2"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,None,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, params["freq"], params["theta"], sigma_theta, 0, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 42
class GaborLayerLogSigmaRepeat(nn.Module):
    """Parametric Gabor layer with particular initialization and optimizing log(sigma^2) insted of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        c_in = inputs.shape[-1]
        features = self.features
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                          nn.initializers.uniform(scale=self.fs/2),
                          (self.features,))
        logsigmax2 = self.param("logsigmax2",
                                log_k_array(k=0.3, arr=1/freq**2),
                                (self.features,))
        logsigmay2 = self.param("logsigmay2",
                                equal_to(0.8*logsigmax2),
                                (self.features,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.features),
                           (self.features,))
        sigma_theta = self.param("sigma_theta",
                           equal_to(theta),
                           (self.features,))
        sigmax2 = jnp.exp(logsigmax2)
        sigmay2 = jnp.exp(logsigmay2)
        # A = self.param("A",
        #                nn.initializers.ones,
        #                (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,None,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, freq, theta, sigma_theta, 0, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = repeat(kernel, "c_out kx ky -> kx ky c_in c_out", c_in=c_in)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax2, sigmay2, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        cov_matrix = jnp.diag(jnp.array([sigmax2, sigmay2]))
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(jnp.linalg.det(cov_matrix))), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigma_theta = params["theta"]
        sigmax2 = jnp.exp(params["logsigmax2"])
        sigmay2 = jnp.exp(params["logsigmay2"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,None,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, params["freq"], params["theta"], sigma_theta, 0, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
        kernel = repeat(kernel, "c_out kx ky -> kx ky c_in c_out", c_in=c_in)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 46
class CenterSurroundLogSigma(nn.Module):
    """Parametric center surround layer that optimizes log(sigma) instead of sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        logsigma = self.param("logsigma",
                           bounded_uniform(minval=-2.2, maxval=-1.7),
                           (self.features*inputs.shape[-1],))
        logsigma2 = self.param("logsigma2",
                           bounded_uniform(minval=-2.2, maxval=-1.7),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigma = jnp.exp(logsigma)
        sigma2 = jnp.exp(logsigma2)
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, sigma2, A, self.normalize_prob, self.normalize_energy)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    # @staticmethod
    # def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
    #     # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
    #     A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma), 1.)
    #     return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
    
    @staticmethod
    def center_surround(x, y, xmean, ymean, sigma, sigma2, A=1, normalize_prob=True, normalize_energy=False):
        def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
            A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
            return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        g1 = gaussian(x, y, xmean, ymean, sigma, 1, normalize_prob)
        g2 = gaussian(x, y, xmean, ymean, sigma2, 1, normalize_prob)
        g = g1-g2
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm
    
    # @staticmethod
    # def center_surround(x, y, xmean, ymean, sigma,  K, A=1, normalize_prob=True):
    #     return (1/(2*jnp.pi*sigma**2))*(jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2)) - (1/(K**2))*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*(K*sigma)**2)))

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["logsigma"]), jnp.exp(params["params"]["logsigma2"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 49
class CenterSurroundLogSigmaK(nn.Module):
    """Parametric center surround layer that optimizes log(sigma) instead of sigma and has a factor K instead of a second sigma."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    normalize_prob: bool = True
    normalize_energy: bool = True
    normalize_sum: bool = True

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        logsigma = self.param("logsigma",
                           bounded_uniform(minval=-2.2, maxval=-1.7),
                           (self.features*inputs.shape[-1],))
        K = self.param("K",
                           displaced_normal(mean=1.1, stddev=0.1),
                           (self.features*inputs.shape[-1],))
        A = self.param("A",
                       nn.initializers.ones,
                       (self.features*inputs.shape[-1],))
        sigma = jnp.exp(logsigma)
        sigma2 = K*sigma
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, sigma2, A, self.normalize_prob, self.normalize_energy, self.normalize_sum)
            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
            kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    # @staticmethod
    # def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
    #     # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.
    #     A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma), 1.)
    #     return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
    
    @staticmethod
    def center_surround(x, y, xmean, ymean, sigma, sigma2, A=1, normalize_prob=True, normalize_energy=False, normalize_sum=False):
        def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):
            A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma**2), 1.)
            return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))
        g1 = gaussian(x, y, xmean, ymean, sigma, 1, normalize_prob)
        g2 = gaussian(x, y, xmean, ymean, sigma2, 1, normalize_prob)
        g = g1 - g2
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        A_sum = jnp.where(normalize_sum, g.sum(axis=(0,1), keepdims=True), 1.)
        g = g/A_sum
        return A*g/E_norm
    
    # @staticmethod
    # def center_surround(x, y, xmean, ymean, sigma,  K, A=1, normalize_prob=True):
    #     return (1/(2*jnp.pi*sigma**2))*(jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2)) - (1/(K**2))*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*(K*sigma)**2)))

    def return_kernel(self, params, c_in):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.center_surround, in_axes=(None,None,None,None,0,0,0,None,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params["params"]["logsigma"]), params["params"]["K"]*jnp.exp(params["params"]["logsigma"]), params["params"]["A"], self.normalize_prob, self.normalize_energy)
        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))
        kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=c_in, c_out=self.features)
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 61
class GaborLayer_(nn.Module):
    """Parametric Gabor layer with particular initialization."""
    # features: int
    n_scales: int
    n_orientations: int
    # n_phases: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    phase: Sequence[float] = jnp.array([0., jnp.pi/2.])

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        features = self.n_scales * self.n_orientations * len(self.phase)
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                           freq_scales_init(n_scales=self.n_scales, fs=self.fs),
                           (self.n_scales,))
        sigmax = self.param("sigmax",
                           k_array(k=0.4, arr=freq),
                           (self.n_scales,))
        sigmay = self.param("sigmay",
                           equal_to(1.5*sigmax),
                           (self.n_scales,))
        # theta = self.param("theta",
        #                    nn.initializers.uniform(scale=jnp.pi),
        #                    (self.n_scales*self.n_orientations,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        sigma_theta = self.param("sigma_theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        # sigma_theta = theta - jnp.pi/2.
        # A = self.param("A",
        #                nn.initializers.ones,
        #                (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None,None), out_axes=0)
            # kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,0,None,None,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, freq, theta, sigma_theta, self.phase, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = rearrange(kernel, "phases rots fs_sigmas kx ky -> kx ky (phases rots fs_sigmas)")
            kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=kernel.shape[-1])
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax, sigmay, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        sigma_vector = jnp.array([sigmax, sigmay])
        cov_matrix = jnp.diag(sigma_vector)**2
        det_cov_matrix = jnp.linalg.det(cov_matrix)
        # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigmax, sigmay = jnp.exp(params["sigmax"]), jnp.exp(params["sigmay"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,None,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,0,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, params["sigmax"], params["sigmay"], params["freq"], params["theta"], params["sigma_theta"], self.phase, 1, self.normalize_prob, self.normalize_energy)
        # kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
        kernel = rearrange(kernel, "rots fs sigmas kx ky -> kx ky (rots fs sigmas)")
        kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=c_in, c_out=kernel.shape[-1])
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 62
class GaborLayerLogSigma_(nn.Module):
    """Parametric Gabor layer with particular initialization and optimizing log(sigma^2) insted of sigma."""
    # features: int
    n_scales: int
    n_orientations: int
    # n_phases: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    phase: Sequence[float] = jnp.array([0., jnp.pi/2.])

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        features = self.n_scales * self.n_orientations * len(self.phase)
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                           freq_scales_init(n_scales=self.n_scales, fs=self.fs),
                           (self.n_scales,))
        # sigmax = self.param("sigmax",
        #                    k_array(k=0.4, arr=freq),
        #                    (self.n_scales,))
        # sigmay = self.param("sigmay",
        #                    equal_to(1.5*sigmax),
        #                    (self.n_scales,))
        logsigmax2 = self.param("logsigmax2",
                                log_k_array(k=0.5, arr=1/freq**2),
                                (self.n_scales,))
        logsigmay2 = self.param("logsigmay2",
                                equal_to(0.8*logsigmax2),
                                (self.n_scales,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        sigma_theta = self.param("sigma_theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        sigmax2, sigmay2 = jnp.exp(logsigmax2), jnp.exp(logsigmay2)
        # A = self.param("A",
        #                nn.initializers.ones,
        #                (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, freq, theta, sigma_theta, self.phase, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = rearrange(kernel, "phases rots fs_sigmas kx ky -> kx ky (phases rots fs_sigmas)")
            kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=kernel.shape[-1])
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax2, sigmay2, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        cov_matrix = jnp.diag(jnp.array([sigmax2, sigmay2]))
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(jnp.linalg.det(cov_matrix))), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigmax2, sigmay2 = jnp.exp(params["logsigmax2"]), jnp.exp(params["logsigmay2"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, params["freq"], params["theta"], params["sigma_theta"], self.phase, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
        # kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
        kernel = rearrange(kernel, "rots fs sigmas kx ky -> kx ky (rots fs sigmas)")
        kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=c_in, c_out=kernel.shape[-1])
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 63
class GaborLayerLogSigmaCoupled_(nn.Module):
    """Parametric Gabor layer with particular initialization and optimizing log(sigma^2) insted of sigma."""
    # features: int
    n_scales: int
    n_orientations: int
    # n_phases: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    phase = jnp.array([0., jnp.pi/2.])

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        features = self.n_scales * self.n_orientations * len(self.phase)
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                           freq_scales_init(n_scales=self.n_scales, fs=self.fs),
                           (self.n_scales,))
        logsigma2 = self.param("logsigma2",
                                log_k_array(k=0.3, arr=1/freq**2),
                                (self.n_scales,))
        # logsigmay2 = self.param("logsigmay2",
        #                         equal_to(0.8*logsigmax2),
        #                         (self.n_scales,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        sigma_theta = theta
        sigmax2 = jnp.exp(logsigma2)
        sigmay2 = 4*sigmax2
        # A = self.param("A",
        #                nn.initializers.ones,
        #                (self.features*inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, freq, theta, sigma_theta, self.phase, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = rearrange(kernel, "phases rots fs_sigmas kx ky -> kx ky (phases rots fs_sigmas)")
            kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=kernel.shape[-1])
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, sigmax2, sigmay2, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        cov_matrix = jnp.diag(jnp.array([sigmax2, sigmay2]))
        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(jnp.linalg.det(cov_matrix))), 1.)
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigma_theta = params["theta"]
        sigmax2 = jnp.exp(params["logsigma2"])
        sigmay2 = 4*sigmax2
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax2, sigmay2, params["freq"], params["theta"], sigma_theta, self.phase, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
        # kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
        kernel = rearrange(kernel, "rots fs sigmas kx ky -> kx ky (rots fs sigmas)")
        kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=c_in, c_out=kernel.shape[-1])
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 69
class GaborLayerGamma_(nn.Module):
    """Parametric Gabor layer with particular initialization."""
    # features: int
    n_scales: int
    n_orientations: int
    # n_phases: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency
    phase: Sequence[float] = jnp.array([0., jnp.pi/2.])

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False
    train_A: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        features = self.n_scales * self.n_orientations * len(self.phase)
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], features))
        freq = self.param("freq",
                           freq_scales_init(n_scales=self.n_scales, fs=self.fs),
                           (self.n_scales,))
        gammax = self.param("gammax",
                           k_array(k=0.4, arr=1/(freq**0.8)),
                           (self.n_scales,))
        # gammay = self.param("gammay",
        #                    k_array(k=0.4, arr=1/(freq**0.8)),
        #                    (self.n_scales,))
        gammay = self.param("gammay",
                            equal_to(gammax*0.8),
                            (self.n_scales,))
        theta = self.param("theta",
                           linspace(start=0, stop=jnp.pi, num=self.n_orientations),
                           (self.n_orientations,))
        # sigma_theta = self.param("sigma_theta",
        #                    linspace(start=0, stop=jnp.pi, num=self.n_orientations),
        #                    (self.n_orientations,))
        sigma_theta = self.param("sigma_theta",
                                  equal_to(theta),
                                  (self.n_orientations,))
        A = self.param("A",
                       nn.initializers.ones_init(),
                       (inputs.shape[-1], self.n_scales*self.n_orientations*len(self.phase))) if self.train_A else jnp.ones(shape=(inputs.shape[-1], self.n_scales*self.n_orientations*len(self.phase)))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None,None), out_axes=0)
            kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, gammax, gammay, freq, theta, sigma_theta, self.phase, 1, self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = rearrange(kernel, "phases rots fs_sigmas kx ky -> kx ky (phases rots fs_sigmas)")
            kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=kernel.shape[-1])
            kernel = kernel * A[None,None,:,:]
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, gammax, gammay, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        gamma_vector = jnp.array([gammax, gammay])
        inv_cov_matrix = jnp.diag(gamma_vector)**2
        # det_cov_matrix = 1/jnp.linalg.det(cov_matrix)
        # # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.
        # A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)
        A_norm = 1.
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ inv_cov_matrix @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        sigmax, sigmay = jnp.exp(params["sigmax"]), jnp.exp(params["sigmay"])
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,None,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,0,None,None,None,None,None,None), out_axes=0)
        kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, params["sigmax"], params["sigmay"], params["freq"], params["theta"], params["sigma_theta"], self.phase, 1, self.normalize_prob, self.normalize_energy)
        # kernel = rearrange(kernel, "(c_in c_out) kx ky -> kx ky c_in c_out", c_in=inputs.shape[-1], c_out=self.features)
        kernel = rearrange(kernel, "rots fs sigmas kx ky -> kx ky (rots fs sigmas)")
        kernel = repeat(kernel, "kx ky c_out -> kx ky c_in c_out", c_in=c_in, c_out=kernel.shape[-1])
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 81
class GaborLayerGammaRepeat(nn.Module):
    """Parametric Gabor layer with particular initialization."""
    features: int
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.zeros_init()
    use_bias: bool = False
    xmean: float = 0.5
    ymean: float = 0.5
    fs: float = 1 # Sampling frequency

    normalize_prob: bool = True
    normalize_energy: bool = False
    zero_mean: bool = False
    train_A: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 return_freq=False,
                 return_theta=False,
                 ):
        is_initialized = self.has_variable("precalc_filter", "kernel")
        precalc_filters = self.variable("precalc_filter",
                                        "kernel",
                                        jnp.zeros,
                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))
        freq = self.param("freq",
                          nn.initializers.uniform(scale=self.fs/2),
                          (self.features,))
        gammax = self.param("gammax",
                           k_array(k=1., arr=1/(freq**0.8)),
                           (self.features,))
        gammay = self.param("gammay",
                            equal_to(gammax*0.8),
                            (self.features,))
        theta = self.param("theta",
                           nn.initializers.uniform(scale=jnp.pi),
                        #    linspace(start=0, stop=jnp.pi, num=self.features),
                           (self.features,))
        sigma_theta = self.param("sigma_theta",
                           nn.initializers.uniform(scale=jnp.pi),
                        #    linspace(start=0, stop=jnp.pi, num=self.features),
                           (self.features,))
        phase = self.param("phase",
                           nn.initializers.uniform(scale=jnp.pi),
                        #    linspace(start=0, stop=jnp.pi, num=self.features),
                           (self.features,))
        A = self.param("A",
                       nn.initializers.ones_init(),
                       (inputs.shape[-1], self.features)) if self.train_A else jnp.ones(shape=(inputs.shape[-1], self.features))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.
        if is_initialized and not train: 
            kernel = precalc_filters.value
        elif is_initialized and train: 
            x, y = self.generate_dominion()
            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,None,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, gammax, gammay, freq, theta, sigma_theta, phase, 1., self.normalize_prob, self.normalize_energy, self.zero_mean)
            kernel = repeat(kernel, "features kx ky -> kx ky c_in features", c_in=inputs.shape[-1])
            kernel = kernel * A[None,None,:,:]
            precalc_filters.value = kernel
        else:
            kernel = precalc_filters.value

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
               (self.strides, self.strides),
               self.padding)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        if return_freq and return_theta:
            return outputs + bias, freq, theta
        elif return_freq and not return_theta:
            return outputs + bias, freq
        elif not return_freq and return_theta:
            return outputs + bias, theta
        else:
            return outputs + bias

    @staticmethod
    def gabor(x, y, xmean, ymean, gammax, gammay, freq, theta, sigma_theta, phase, A=1, normalize_prob=True, normalize_energy=False, zero_mean=False):
        x, y = x-xmean, y-ymean
        ## Obtain the normalization coeficient
        gamma_vector = jnp.array([gammax, gammay])
        inv_cov_matrix = jnp.diag(gamma_vector)**2
        # det_cov_matrix = 1/jnp.linalg.det(cov_matrix)
        # # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.
        # A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)
        A_norm = 1.
        
        ## Rotate the sinusoid
        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],
                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ inv_cov_matrix @ jnp.transpose(rotation_matrix)
        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y
        distance = x * x_r_1 + y * y_r_1
        g = A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)) + phase)
        g = jnp.where(zero_mean, g - g.mean(), g)
        E_norm = jnp.where(normalize_energy, jnp.sqrt(jnp.sum(g**2)), 1.)
        return A*g/E_norm

    def return_kernel(self, params, c_in=3):
        x, y = self.generate_dominion()
        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,None,None,None), out_axes=0)(x, y, self.xmean, self.ymean, 1/params["gammax"], 1/params["gammay"], params["freq"], params["theta"], params["sigma_theta"], params["phase"], 1, self.normalize_prob, self.normalize_energy)
        kernel = repeat(kernel, "features kx ky -> kx ky c_in features", c_in=c_in)
        kernel = kernel * params["A"][None,None,:,:]
        return kernel
    
    def generate_dominion(self):
        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size), jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size))

# %% ../Notebooks/00_layers.ipynb 89
class JamesonHurvich(nn.Module):
    """Jameson & Hurvich transformation from RGB to ATD."""

    def setup(self):
        self.Mng2xyz = jnp.array([[69.1661, 52.4902, 46.6052],
                                  [39.0454, 115.8404, 16.3118],
                                  [3.3467, 12.6700, 170.1090]])
        self.Mxyz2atd = jnp.array([[0, 1, 0],
                                   [1, -1, 0],
                                   [0, 0.4, -0.4]])

    def __call__(self,
                 inputs, # (B,H,W,C)
                 **kwargs,
                 ):
        outputs = inputs**2
        outputs = inputs @ self.Mng2xyz.T @ self.Mxyz2atd.T
        return outputs

# %% ../Notebooks/00_layers.ipynb 93
def metefot(sec, foto, N, ma):
    ss = foto.shape
    fil = ss[0]
    col = ss[1]
    s = sec.shape
    Nfot = s[1] / col

    if N > Nfot:
        sec = [sec, foto]
    else:
        if ma == 1:
            sec = sec.at[:, (N-1)*col:N*col].set(foto)
    # if incorrect results finish this function.
    return sec

# %% ../Notebooks/00_layers.ipynb 94
def freqspace(N):
    # Returns 2-d frequency range vectors for N[0] x N[1] matrix

    f1 = (jnp.arange(0, N[0], 1)-jnp.floor(N[0]/2))*(2/N[0])
    f2 = (jnp.arange(0, N[1], 1)-jnp.floor(N[1]/2))*(2/N[1])
    F1, F2 = jnp.meshgrid(f1, f2)
    return F1, F2

# %% ../Notebooks/00_layers.ipynb 95
def spatio_temp_freq_domain(Ny, Nx, Nt, fsx, fsy, fst):
    int_x = Nx/fsx # Physical domain
    int_y = Ny/fsy
    int_t = Nt/fst

    x = jnp.zeros((Ny, Nx*Nt)) # Big matrix
    y = jnp.zeros((Ny, Nx*Nt))
    t = jnp.zeros((Ny, Nx*Nt))

    fot_x = jnp.linspace(0, int_x, Nx+1)
    fot_x = fot_x[:-1]
    fot_x = fot_x[None,:].repeat(Ny, 0)

    fot_y = jnp.linspace(0, int_y, Ny+1)
    fot_y = fot_y[:-1]
    fot_y = fot_y[:,None].repeat(Nx, 1)

    fot_t = jnp.ones((Ny, Nx))

    val_t = jnp.linspace(0, int_t, Nt+1)
    val_t = val_t[:-1]

    for i in range(Nt):
        x = metefot(x, fot_x, i+1, 1)
        y = metefot(y, fot_y, i+1, 1)
        t = metefot(t, val_t[i]*fot_t, i+1, 1)

    [fx, fy] = freqspace([Nx, Ny])

    fx = fx*fsx/2
    fy = fy*fsy/2

    ffx = jnp.zeros((Ny, Nx*Nt))
    ffy = jnp.zeros((Ny, Nx*Nt))
    ff_t = jnp.zeros((Ny, Nx*Nt))

    fot_fx = fx
    fot_fy = fy
    fot_t = jnp.ones((Ny, Nx))

    [ft, ft2] = freqspace([Nt, Nt])
    val_t = ft*fst/2

    for i in range(Nt):
        ffx = metefot(ffx, fot_fx, i+1, 1)
        ffy = metefot(ffy, fot_fy, i+1, 1)
        ff_t = metefot(ff_t, val_t[0,:][i]*fot_t, i+1, 1)

    return x, y, t, ffx, ffy, ff_t

# %% ../Notebooks/00_layers.ipynb 96
class CSFFourier(nn.Module):
    """CSF SSO."""
    fs: int = 64
    norm_energy: bool = True

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        alpha_achrom = self.param("alpha_achrom",
                                  equal_to(1.),
                                  (1,))
        alpha_chrom_rg = self.param("alpha_chrom_rg",
                                 equal_to(1.),
                                 (1,))
        alpha_chrom_yb = self.param("alpha_chrom_yb",
                                 equal_to(1.),
                                 (1,))
        beta_achrom = self.param("beta_achrom",
                          equal_to(1.),
                          (1,))
        beta_chrom = self.param("beta_chrom",
                          equal_to(1.),
                          (1,))
        fm = self.param("fm",
                        equal_to(7.28),
                        (1,))
        s = self.param("s",
                       equal_to(1.809),
                       (1,))
        
        b, h, w, c = inputs.shape

        ## 1. Achromatic CSF
        # csf, fx, fy = jax.jit(self.csf_sso, static_argnums=(1,2))(fs=self.fs, Nx=w, Ny=h, alpha=alpha_achrom, beta=beta_achrom, g=330.74, fm=fm, l=0.837, s=s, w=1.0, os=6.664)
        csf, fx, fy = self.csf_sso(fs=self.fs, Nx=w, Ny=h, alpha=alpha_achrom, beta=beta_achrom, g=330.74, fm=fm, l=0.837, s=s, w=1.0, os=6.664)

        # jax.debug.print(f"Nx={w}, Ny={h}, {csf.shape}")
        ## 2. Chromatic CSFs
        # csfrg, csfyb, fx, fy = jax.jit(self.csf_chrom, static_argnums=(1,2))(fs=self.fs, Nx=w, Ny=h, alpha_rg=alpha_chrom_rg, alpha_yb=alpha_chrom_yb, beta=beta_chrom)
        csfrg, csfyb, fx, fy = self.csf_chrom(fs=self.fs, Nx=w, Ny=h, alpha_rg=alpha_chrom_rg, alpha_yb=alpha_chrom_yb, beta=beta_chrom)
        # jax.debug.print(f"Nx={w}, Ny={h}, {csf.shape}, {csfrg.shape}, {csfyb.shape}")

        ## 3. Stack the three CSFs together
        csfs = jnp.stack([csf, csfrg, csfyb], axis=-1)

        ## 4. FFT of the input
        inputs_fft = jnp.fft.fft2(inputs, axes=(1,2))
        inputs_fft = jnp.fft.fftshift(inputs_fft)
        
        ## 5. Apply the CSF by multiplying
        E1 = jnp.sum(jnp.ones_like(csfs)**2)#**(1/2)
        E_CSF = jnp.sum(csfs**2)#**(1/2)
        if self.norm_energy: csfs = (csfs/E_CSF)*E1
        inputs_fft = csfs[None,:]*inputs_fft

        ## 6. Return to the original domain
        outputs = jnp.fft.ifft2(jnp.fft.ifftshift(inputs_fft), axes=(1,2))
        outputs = jnp.real(outputs)
        
        return outputs
    
    @staticmethod
    def csf_sso(fs, Nx, Ny, alpha, beta, g=330.74, fm=7.28, l=0.837, s=1.809, w=1.0, os=6.664):

        [_,_,_,fx,fy,_] = spatio_temp_freq_domain(Nx=Nx, Ny=Ny, Nt=1, fsx=fs, fsy=fs, fst=1)
        fx, fy = fx*beta, fy*beta
        f = jnp.sqrt(jnp.clip(fx**2 + fy**2, a_min=0.0001))
        # f = f.at[f == 0].set(0.0001)

        CSFT = g * (jnp.exp(-(f/fm)) - l*jnp.exp(-(f**2/s**2)))
        OE = 1 - w*(4*(1-jnp.exp(-(f/os)))*fx**2 *fy**2)/(f**4)
        CSFSSO = alpha*(CSFT * OE)

        return CSFSSO, fx, fy
    
    @staticmethod
    def csf_chrom(fs, Nx, Ny, alpha_rg, alpha_yb, beta):

        def sigm1d(x,x0,s):
            y = 1/(1+jnp.exp((x-x0)/s))
            return y
        
        def umbinc3(c,cu,k,m,alf,sig):
            umb = (cu-k*cu**m)*(1/(1+jnp.exp(jnp.log10(c/(alf*cu))/sig)))+(k*c**m)*(1-1/(1+jnp.exp(jnp.log10(c/(0.9*cu))/(sig/2))))
            return umb
        
        # def iafrg(f, C, facfrec, nolin):
        #     f = facfrec*f
        #     f = jnp.clip(f, a_min=0.00001)
        #     C = jnp.clip(C, a_min=0.0000001)

        #     lf = len(f)
        #     lc = len(C)

        #     #iaf = np.zeros(lf,lc)
        #     ace = jnp.zeros((lf,lc))
        #     p = [0.0840, 0.8345, 0.6313, 0.2077]

        #     if len(nolin)==1:
        #         nolin = [nolin, nolin]

        #     nolini = nolin
        #     nolin = nolini[0]

        #     if ((nolini[0]==0)&(nolini[1]==1)):
        #         nolin=1


        #     if nolin==1:
        #         for i in range(lf):
        #             cu = 1/(100*2537.9*sigm1d(f[i],-55.94,6.64))
        #             ace[i,:] = umbinc3(C,cu,p[0],p[1],p[2],p[3])

        #         iaf = 1/ace

        #     else:
        #         iaf=100*2537.9*sigm1d(f,-55.94,6.64)
        #         iaf=iaf*jnp.ones((1,len(C)))

        #     csfrg=iaf[0,:]

        #     if ((nolini[0]==0)&(nolini[1]==1)):
        #         s = iaf.shape
        #         iafc = jnp.sum(iaf)
        #         iaf = iafc*jnp.ones((1,s[1]))

        #     return iaf, csfrg
        
        def iafrg(f, C, facfrec, nolin, lenf, lenC):
            f = facfrec*f
            f = jnp.clip(f, a_min=0.00001)
            C = jnp.clip(C, a_min=0.0000001)

            # lf = len(f)
            # lc = len(C)
            lf, lc = lenf, lenC

            #iaf = np.zeros(lf,lc)
            ace = jnp.zeros((lf,lc))
            p = [0.0840, 0.8345, 0.6313, 0.2077]

            # if len(nolin)==1:
            #     nolin = [nolin, nolin]

            nolini = nolin
            nolin = nolini[0]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            #     nolin=1


            # if nolin==1:
            #     for i in range(lf):
            #         cu = 1/(100*2537.9*sigm1d(f[i],-55.94,6.64))
            #         ace[i,:] = umbinc3(C,cu,p[0],p[1],p[2],p[3])

            #     iaf = 1./ace

            # else:
            iaf=100*2537.9*sigm1d(f,-55.94,6.64)
            iaf=iaf*jnp.ones((1,len(C)))

            csfrg=iaf[0,:]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            s = iaf.shape
            iafc = jnp.sum(iaf)
            iaf = iafc*jnp.ones((1,s[1]))

            return iaf, csfrg

        def iafyb(f, C, facfrec, nolin, lenf, lenC):
            f = facfrec*f
            f = jnp.clip(f, a_min=0.00001)
            C = jnp.clip(C, a_min=0.0000001)

            # lf = len(f)
            # lc = len(C)
            lf, lc = lenf, lenC

            #iaf = np.zeros(lf,lc)
            ace = jnp.zeros((lf,lc))
            p = [0.1611, 1.3354, 0.3077, 0.7746]

            # if len(nolin)==1:
            #     nolin = [nolin, nolin]

            nolini = nolin
            nolin = nolini[0]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            #     nolin=1

            # if nolin==1:
            #     for i in range(lf):
            #         cu=1/(100*719.7*sigm1d(f[i],-31.72,4.13))
            #         ace[i,:]=umbinc3(C,cu,p[0],p[1],p[2],p[3])

            #     iaf = 1/ace

            # else:
            iaf=100*719.7*sigm1d(f,-31.72,4.13)
            iaf=iaf*jnp.ones((1,len(C)))

            csfyb=iaf[0,:]

            # if ((nolini[0]==0)&(nolini[1]==1)):
            s=iaf.shape
            iafc=jnp.sum(iaf)
            iaf=iafc*jnp.ones((1,s[1]))
            
            return iaf, csfyb

        [_,_,_,fx,fy,_] = spatio_temp_freq_domain(Nx=Nx, Ny=Ny, Nt=1, fsx=fs, fsy=fs, fst=1)
        fx, fy = fx*beta, fy*beta
        # f = jnp.sqrt(fx**2 + fy**2)
        f = jnp.sqrt(jnp.clip(fx**2 + fy**2, a_min=0.0001))
        #f[f == 0] = 0.0001

        csfrg = jnp.zeros((Ny,Nx))
        csfyb = jnp.zeros((Ny,Nx))

        for i in range(Ny):
            [iaf_rg, csf_c] = iafrg(f[i,:], jnp.array([0.1]), 1., jnp.array([0., 0., 0.]), len(f[i,:]), len(jnp.array([0.1])))
            # jax.debug.print(f"{csfrg.shape} / {csf_c.shape}")
            csfrg = csfrg.at[i,:].set(csf_c)

            [iaf_yb, csf_c] = iafyb(f[i,:], jnp.array([0.1]), 1., jnp.array([0., 0., 0.]), len(f[i,:]), len(jnp.array([0.1])))
            csfyb = csfyb.at[i,:].set(csf_c)

        fact_rg = 0.75
        fact_yb = 0.55
        max_CSF_achro = 201.3

        csfrg = fact_rg*max_CSF_achro*csfrg/jnp.max(csfrg)
        csfyb = fact_yb*max_CSF_achro*csfyb/jnp.max(csfyb)

        return alpha_rg*csfrg, alpha_yb*csfyb, fx, fy

# %% ../Notebooks/00_layers.ipynb 128
def pad_same_from_kernel_size(inputs, # Input to be padded.
                              kernel_size: Union[int, Sequence[int]], # Kernel size.
                              mode: str, # Convolution type.
                              ):
    """Pads `inputs` so that a convolution of `kernel_size` maintains the same size after the operation."""
    if isinstance(kernel_size, int): kernel_size = (kernel_size, kernel_size)
    return jnp.pad(inputs,
                   [[0,0],
                    [(kernel_size[0]-1)//2, (kernel_size[0]-1)//2],
                    [(kernel_size[1]-1)//2, (kernel_size[1]-1)//2],
                    [0,0]],
                    mode=mode)

# %% ../Notebooks/00_layers.ipynb 131
class GDN(nn.Module):
    """Generalized Divisive Normalization."""
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    apply_independently: bool = False
    # kernel_init: Callable = nn.initializers.lecun_normal()
    kernel_init: Callable = mean()
    bias_init: Callable = nn.initializers.ones_init()
    alpha: float = 2.
    epsilon: float = 1/2 # Exponential of the denominator
    eps: float = 1e-6 # Numerical stability in the denominator
    return_coef: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 ):
        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input
                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, 
                        strides=self.strides, 
                        padding=self.padding,
                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,
                        kernel_init=self.kernel_init, 
                        bias_init=self.bias_init)(inputs**self.alpha)
        coef = 1 / (jnp.clip(denom, a_min=1e-5)**self.epsilon + self.eps)
        if self.return_coef:
            return coef*inputs, coef
        else:
            return coef*inputs

# %% ../Notebooks/00_layers.ipynb 133
class GDNGamma(nn.Module):
    """Generalized Divisive Normalization."""
    # kernel_size: Union[int, Sequence[int]]
    # strides: int = 1
    # padding: str = "SAME"
    # apply_independently: bool = False
    # kernel_init: Callable = nn.initializers.lecun_normal()
    kernel_init: Callable = mean()
    bias_init: Callable = nn.initializers.ones_init()
    alpha: float = 2.
    epsilon: float = 1/2 # Exponential of the denominator
    eps: float = 1e-6 # Numerical stability in the denominator
    return_coef: bool = False

    @nn.compact
    def __call__(self,
                 inputs,
                 ):
        kernel = self.param("kernel",
                    self.kernel_init,
                    (1,))
        bias = self.param("bias",
                    self.bias_init,
                    (1,))
        denom = bias + kernel*inputs**2
        
        coef = 1 / (jnp.clip(denom, a_min=1e-5)**self.epsilon + self.eps)
        if self.return_coef:
            return coef*inputs, coef
        else:
            return coef*inputs

# %% ../Notebooks/00_layers.ipynb 137
class GDNGaussian(nn.Module):
    """Generalized Divisive Normalization with a Gaussian kernel."""
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    apply_independently: bool = False
    fs: Union[int, None] = None
    # feature_group_count: int = 1
    kernel_init: Callable = nn.initializers.lecun_normal()
    bias_init: Callable = nn.initializers.ones_init()
    alpha: float = 2.
    epsilon: float = 1/2 # Exponential of the denominator
    eps: float = 1e-6 # Numerical stability in the denominator
    normalize_prob: bool = False
    normalize_energy: bool = True
    return_coef: bool = False

    @nn.compact
    def __call__(self,
                inputs,
                **kwargs,
                ):
        fs = self.kernel_size if self.fs is None else self.fs
        denom = GaussianLayerGamma(features=inputs.shape[-1], # Same output channels as input
                                   kernel_size=self.kernel_size,# if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, 
                                   strides=self.strides, 
                                   padding="VALID",
                                   feature_group_count=inputs.shape[-1] if self.apply_independently else 1,
                                   fs=fs,
                                   xmean=self.kernel_size/fs/2,
                                   ymean=self.kernel_size/fs/2,
                                   use_bias=True,
                                   normalize_prob=self.normalize_prob,
                                   normalize_energy=self.normalize_energy)(pad_same_from_kernel_size(inputs**self.alpha, kernel_size=self.kernel_size, mode=self.padding), **kwargs)
        coef = 1 / (jnp.clip(denom, a_min=1e-5)**self.epsilon + self.eps)
        if self.return_coef:
            return coef*inputs, coef
        else:
            return coef*inputs

# %% ../Notebooks/00_layers.ipynb 139
class ClippedModule(nn.Module):
    layer: nn.Module
    a_min: float = -jnp.inf
    a_max: float = jnp.inf

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        return jnp.clip(self.layer(inputs, **kwargs), a_min=self.a_min, a_max=self.a_max)

# %% ../Notebooks/00_layers.ipynb 140
class GDNStar(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    kernel_init: Callable = nn.initializers.ones_init()
    bias_init: Callable = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H(inputs**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        return coef*inputs/denom

# %% ../Notebooks/00_layers.ipynb 149
class GDNStarSign(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        inputs_sign = jnp.sign(inputs)
        inputs = jnp.abs(inputs)
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H(inputs**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        return coef*inputs*inputs_sign/denom

# %% ../Notebooks/00_layers.ipynb 157
class GDNDisplacement(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        inputs_mean = inputs.mean(axis=(1,2), keepdims=True)
        inputs_mean = jnp.ones_like(inputs)*inputs_mean
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        # inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H((inputs-inputs_mean)**self.alpha), a_min=1e-5)**self.epsilon
        # coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        coef = 1.
        return coef*(inputs-inputs_mean)/denom

# %% ../Notebooks/00_layers.ipynb 162
from paramperceptnet.layers import GDNSpatioChromaFreqOrient

class GDNControl(nn.Module):
    """---"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    fs: int = 1
    normalize_prob: bool = False
    normalize_energy: bool = False
    normalize_sum: bool = True
    # K: Sequence[float] = 1.

    @nn.compact
    def __call__(self,
                 inputs,
                 fmean,
                 theta_mean,
                 train=False,
                 **kwargs,
                ):
        is_initialized_star = self.has_variable("batch_stats", "inputs_star")
        is_initialized_K = self.has_variable("batch_stats", "K")
        inputs_star = self.variable("batch_stats", "inputs_star", jnp.ones, (1,1,1,inputs.shape[-1]))
        K = self.variable("batch_stats", "K", jnp.ones, (1,1,1,inputs.shape[-1]))
    
        if is_initialized_star and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95, axis=(1,2), keepdims=True))/2

        if is_initialized_K and train:
            K.value = (K.value + jnp.quantile(jnp.abs(inputs), q=0.95, axis=(1,2), keepdims=True))/2

        H = GDNSpatioChromaFreqOrient(self.kernel_size, fs=self.fs, apply_independently=self.apply_independently, padding="symmetric",
                                      normalize_prob=self.normalize_prob, normalize_energy=self.normalize_energy, normalize_sum=self.normalize_sum,
                                      alpha=self.alpha, epsilon=self.epsilon)
        # inputs_star = jnp.abs(inputs).mean(axis=(0,1,2), keepdims=True)
        rh = H(inputs, fmean, theta_mean, train=train)
        lh = H(jnp.broadcast_to(inputs_star.value, inputs.shape), fmean, theta_mean, train=train)
        return (K.value/lh)*rh

# %% ../Notebooks/00_layers.ipynb 252
class GDNStarDisplacement(nn.Module):
    """GDN variation that forces the output to be 1 when the input is x^*"""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    inputs_star: Union[float, Sequence[float]] = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        inputs_mean = inputs.mean(axis=(1,2), keepdims=True)
        inputs_mean = jnp.ones_like(inputs)*inputs_mean
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star = jnp.ones_like(inputs)*self.inputs_star
        denom = jnp.clip(H((inputs-inputs_mean)**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star
        # coef = 1.
        return coef*(inputs-inputs_mean)/denom

# %% ../Notebooks/00_layers.ipynb 258
class GDNStarRunning(nn.Module):
    """GDN variation where x^* is obtained as a running mean of the previously obtained values."""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    # inputs_star: float = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    bias_init: Callable = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        is_initialized = self.has_variable("batch_stats", "inputs_star")
        # inputs_star = self.variable("batch_stats", "inputs_star", lambda x: x, jnp.quantile(inputs, q=0.95))
        inputs_star = self.variable("batch_stats", "inputs_star", jnp.ones, (1,))
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1, bias_init=self.bias_init)
        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value
        denom = jnp.clip(H((inputs)**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star_**self.alpha), a_min=1e-5)**self.epsilon)#/inputs_star_
        if is_initialized and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2
        return coef*inputs/denom

# %% ../Notebooks/00_layers.ipynb 265
class GDNStarDisplacementRunning(nn.Module):
    """GDN variation where x^* is obtained as a running mean of the previously obtained values."""

    kernel_size: Sequence[int]
    apply_independently: bool = False
    # inputs_star: float = 1.
    alpha: float = 2.
    epsilon: float = 1/2
    # kernel_init = nn.initializers.ones_init()
    # bias_init = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        is_initialized = self.has_variable("batch_stats", "inputs_star")
        # inputs_star = self.variable("batch_stats", "inputs_star", lambda x: x, jnp.quantile(inputs, q=0.95))
        inputs_star = self.variable("batch_stats", "inputs_star", jnp.ones, (1,))
        inputs_mean = inputs.mean(axis=(1,2), keepdims=True)
        inputs_mean = jnp.ones_like(inputs)*inputs_mean
        H = nn.Conv(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, feature_group_count=inputs.shape[-1] if self.apply_independently else 1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)
        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value
        denom = jnp.clip(H((inputs-inputs_mean)**self.alpha), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star_**self.alpha), a_min=1e-5)**self.epsilon)/inputs_star_
        if is_initialized and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2
        return coef*(inputs-inputs_mean)/denom

# %% ../Notebooks/00_layers.ipynb 273
class FreqGaussian(nn.Module):
    """(1D) Gaussian interaction between frequencies."""
    use_bias: bool = False
    strides: int = 1
    padding: str = "SAME"
    bias_init: Callable = nn.initializers.zeros_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 fmean,
                 **kwargs,
                 ):
        sigma = self.param("sigma",
                           k_array(0.4, arr=1/fmean),
                           (inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (len(fmean),))
        else: bias = 0.
        n_groups = inputs.shape[-1] // len(fmean)
        kernel = jax.vmap(self.gaussian, in_axes=(None,0,0,None), out_axes=1)(fmean, fmean, sigma, 1)
        kernel = kernel[None,None,:,:]
        kernel = jnp.tile(kernel, reps=n_groups)

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(
                jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
                jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
                (self.strides, self.strides),
                self.padding,
                feature_group_count=n_groups)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(f, fmean, sigma, A=1):
        return A*jnp.exp(-((f-fmean)**2)/(2*sigma**2))

# %% ../Notebooks/00_layers.ipynb 274
class FreqGaussianGamma(nn.Module):
    """(1D) Gaussian interaction between frequencies optimizing gamma = 1/sigma instead of sigma."""
    use_bias: bool = False
    strides: int = 1
    padding: str = "SAME"
    bias_init: Callable = nn.initializers.zeros_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 fmean,
                 **kwargs,
                 ):
        gamma = self.param("gamma",
                           k_array(1/0.4, arr=fmean),
                           (inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (len(fmean),))
        else: bias = 0.
        n_groups = inputs.shape[-1] // len(fmean)
        kernel = jax.vmap(self.gaussian, in_axes=(None,0,0,None), out_axes=1)(fmean, fmean, gamma, 1)
        kernel = kernel[None,None,:,:]
        kernel = jnp.tile(kernel, reps=n_groups)

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(
                jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
                jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
                (self.strides, self.strides),
                self.padding,
                feature_group_count=n_groups)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(f, fmean, gamma, A=1):
        return A*jnp.exp(-((gamma**2)*(f-fmean)**2)/(2))

# %% ../Notebooks/00_layers.ipynb 284
def wrapTo180(angle, # Deg
              ):
    """Wraps an angle to the range [-180, 180]."""
    angle =  angle % 360
    angle = (angle + 360) % 360        
    return jnp.where(angle>180, angle-360, angle)

# %% ../Notebooks/00_layers.ipynb 286
def process_angles(angle1, # Deg.
                   angle2, # Deg
                   ):
    """Takes two angles as input and outputs their difference making all necessary assumptions."""
    dif = angle1 - angle2
    dif2 = dif + 180
    return jnp.min(jnp.stack([jnp.abs(wrapTo180(dif)), jnp.abs(wrapTo180(dif2))]), axis=0)

# %% ../Notebooks/00_layers.ipynb 288
class OrientGaussian(nn.Module):
    """(1D) Gaussian interaction between orientations."""
    use_bias: bool = False
    strides: int = 1
    padding: str = "SAME"
    bias_init: Callable = nn.initializers.zeros_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 theta_mean,
                 **kwargs,
                 ):
        sigma = self.param("sigma",
                        #    equal_to([jnp.pi/4]*len(theta_mean)),
                           equal_to([30]*len(theta_mean)),
                           (inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (len(fmean),))
        else: bias = 0.
        n_groups = inputs.shape[-1] // len(theta_mean)
        kernel = jax.vmap(self.gaussian, in_axes=(None,0,0,None), out_axes=1)(theta_mean, theta_mean, sigma, 1)
        kernel = kernel[None,None,:,:]
        kernel = jnp.tile(kernel, reps=n_groups)

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(
                jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
                jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
                (self.strides, self.strides),
                self.padding,
                feature_group_count=n_groups)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(theta, theta_mean, sigma, A=1):
        return A*jnp.exp(-(process_angles(theta, theta_mean)**2)/(2*sigma**2))

# %% ../Notebooks/00_layers.ipynb 289
class OrientGaussianGamma(nn.Module):
    """(1D) Gaussian interaction between orientations optimizing gamma = 1/sigma instead of sigma."""
    use_bias: bool = False
    strides: int = 1
    padding: str = "SAME"
    bias_init: Callable = nn.initializers.zeros_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 theta_mean,
                 **kwargs,
                 ):
        gamma = self.param("gamma",
                        #    equal_to([jnp.pi/4]*len(theta_mean)),
                           equal_to([1/30]*len(theta_mean)),
                           (inputs.shape[-1],))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (len(fmean),))
        else: bias = 0.
        n_groups = inputs.shape[-1] // len(theta_mean)
        kernel = jax.vmap(self.gaussian, in_axes=(None,0,0,None), out_axes=1)(theta_mean, theta_mean, gamma, 1)
        kernel = kernel[None,None,:,:]
        kernel = jnp.tile(kernel, reps=n_groups)

        ## Add the batch dim if the input is a single element
        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False
        else: had_batch = True
        outputs = lax.conv_general_dilated(
                jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor
                jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor
                (self.strides, self.strides),
                self.padding,
                feature_group_count=n_groups)
        ## Move the channels back to the last dim
        outputs = jnp.transpose(outputs, (0,2,3,1))
        if not had_batch: outputs = outputs[0]
        return outputs + bias

    @staticmethod
    def gaussian(theta, theta_mean, gamma, A=1):
        return A*jnp.exp(-((gamma**2)*process_angles(theta, theta_mean)**2)/(2))

# %% ../Notebooks/00_layers.ipynb 303
class GDNGaussianStarRunning(nn.Module):
    """GDN variation where x^* is obtained as a running mean of the previously obtained values."""

    kernel_size: int
    inputs_star: float = 1.
    outputs_star: Union[None, float] = None
    fs: int = 1
    apply_independently: bool = False
    alpha: float = 2.
    epsilon: float = 1/2
    bias_init: Callable = nn.initializers.ones_init()

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 **kwargs,
                 ):
        # inputs_sign = jnp.sign(inputs)
        # inputs = jnp.abs(inputs)
        is_initialized = self.has_variable("batch_stats", "inputs_star")
        # inputs_star = self.variable("batch_stats", "inputs_star", lambda x: x, jnp.quantile(inputs, q=0.95))
        inputs_star = self.variable("batch_stats", "inputs_star", lambda x: jnp.ones(x)*self.inputs_star, (1,))
        if is_initialized and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2
        H = GaussianLayerGamma(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, bias_init=self.bias_init, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY)
        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value
        denom = jnp.clip(H(inputs**self.alpha, train=train), a_min=1e-5)**self.epsilon
        coef = (jnp.clip(H(inputs_star_**self.alpha, train=train), a_min=1e-5)**self.epsilon)#/inputs_star_
        if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star
        
        return coef*inputs/denom

# %% ../Notebooks/00_layers.ipynb 305
class GDNSpatioFreqOrient(nn.Module):
    """Generalized Divisive Normalization."""
    kernel_size: Union[int, Sequence[int]]
    strides: int = 1
    padding: str = "SAME"
    inputs_star: float = 1.
    outputs_star: Union[None, float] = None
    fs: int = 1
    apply_independently: bool = False
    bias_init: Callable = nn.initializers.ones_init()
    alpha: float = 2.
    epsilon: float = 1/2 # Exponential of the denominator
    eps: float = 1e-6 # Numerical stability in the denominator

    @nn.compact
    def __call__(self,
                 inputs,
                 fmean,
                 theta_mean,
                 train=False,
                 ):
        b, h, w, c = inputs.shape
        bias = self.param("bias",
                          #equal_to(inputs_star/10),
                          self.bias_init,
                          (c,))
        is_initialized = self.has_variable("batch_stats", "inputs_star")
        inputs_star = self.variable("batch_stats", "inputs_star", lambda x: jnp.ones(x)*self.inputs_star, (len(self.inputs_star),))
        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value
        GL = GaussianLayerGamma(features=c, kernel_size=self.kernel_size, strides=self.strides, padding=self.padding, fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, use_bias=False, feature_group_count=c)
        FG = FreqGaussian()
        OG = OrientGaussian()
        outputs = GL(inputs**self.alpha, train=train)#/(self.kernel_size**2)
        outputs = FG(outputs, fmean=fmean)
        ## Reshape so that the orientations are the innermost dimmension
        outputs = rearrange(outputs, "b h w (phase theta f) -> b h w (phase f theta)", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)
        outputs = OG(outputs, theta_mean=theta_mean)
        ## Recover original disposition
        denom = rearrange(outputs, "b h w (phase f theta) -> b h w (phase theta f)", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)

        ## Coef
        coef = GL(inputs_star_**self.alpha, train=train)#/(self.kernel_size**2)
        coef = FG(coef, fmean=fmean)
        coef = rearrange(coef, "b h w (phase theta f) -> b h w (phase f theta)", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)
        coef = OG(coef, theta_mean=theta_mean) + bias
        coef = rearrange(coef, "b h w (phase f theta) -> b h w (phase theta f)", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)
        coef = jnp.clip(coef+bias, a_min=1e-5)**self.epsilon
        # coef = inputs_star.value * coef
        if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star

        if is_initialized and train:
            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95, axis=(0,1,2)))/2
        return coef * inputs / (jnp.clip(denom+bias, a_min=1e-5)**self.epsilon + self.eps)

# %% ../Notebooks/00_layers.ipynb 308
class GaborGammaFourier(nn.Module):
    features: int
    fs: int  = 1
    use_bias: bool = False
    norm_energy: bool = True

    @nn.compact
    def __call__(self,
                 inputs,
                 train=False,
                 **kwargs):
        freq = self.param("freq",
                          nn.initializers.uniform(scale=self.fs/2),
                          (self.features,))
        gamma = self.param("gamma",
                        #    k_array(k=0.5, arr=freq),
                          nn.initializers.uniform(scale=1.),
                        #    nn.initializers.ones_init(),
                           (self.features,))
        theta = self.param("theta",
                           nn.initializers.uniform(scale=jnp.pi),
                           (self.features,))
        if self.use_bias: bias = self.param("bias",
                                            self.bias_init,
                                            (self.features,))
        else: bias = 0.

        fx, fy = self.generate_dominion(inputs.shape[1:-1], fs=self.fs)
        kernel = jax.vmap(self.gaussians, in_axes=(None,None,0,None,0,0), out_axes=-1)(fx, fy, freq, 0., gamma, theta)
        if self.norm_energy:
            kernel = kernel / (kernel**2).sum(axis=(1,2), keepdims=True)**(1/2)
        ## Add empty dims for broadcasting
        outputs = inputs[...,None] * kernel[None,...,None,:]
        ## Sum the broadcasted dim
        outputs = outputs.sum(axis=-2)
        ## Add bias (or not)
        return outputs + bias
    
    @staticmethod
    def gaussians(fx, fy, fxm, fym, gamma, theta):
        def gaussian(fx, fy, fxm, fym, gamma):
            return jnp.exp(-(((fx-fxm)**2 + (fy-fym)**2)*gamma**2)/2)
        def rotate_dom(fx, fy, theta):
            fx_r = fx*jnp.cos(theta) - fy*jnp.sin(theta)
            fy_r = fx*jnp.sin(theta) + fy*jnp.cos(theta)
            return fx_r, fy_r
        fxm_r, fym_r = rotate_dom(fxm, fym, theta)
        
        return gaussian(fx, fy, fxm_r, fym_r, gamma) + gaussian(fx, fy, -fxm_r, -fym_r, gamma)

    @staticmethod
    def generate_dominion(input_size, fs):
        return jnp.meshgrid(jnp.linspace(-fs/2,fs/2,num=input_size[1]), jnp.linspace(-fs/2,fs/2,num=input_size[0]))
    
    def return_kernel(self, params, c_in, kernel_shape):
        fx, fy = self.generate_dominion(kernel_shape, fs=self.fs)
        kernel = jax.vmap(self.gaussians, in_axes=(None,None,0,None,0,0), out_axes=-1)(fx, fy, params["freq"], 0., params["gamma"], params["theta"])
        return kernel
