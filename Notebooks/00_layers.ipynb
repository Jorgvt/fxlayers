{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional layers\n",
    "\n",
    "> JaX/Flax implementation of functional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from einops import rearrange\n",
    "\n",
    "from fxlayers.initializers import bounded_uniform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer\n",
    "\n",
    "> First we'll define a base class that will be used by every other functional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFunctional(nn.Module):\n",
    "    \"\"\"Base functional layer.\"\"\"\n",
    "    features: int\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    feature_group_count: int = 1\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "    xmean: float = 0.5\n",
    "    ymean: float = 0.5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 ):\n",
    "        sigma = self.param(\"sigma\",\n",
    "                           nn.initializers.uniform(scale=1),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "\n",
    "        # x, y = jnp.meshgrid(jnp.linspace(0,1,num=self.kernel_size), jnp.linspace(0,1,num=self.kernel_size))\n",
    "        # kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,None), out_axes=-1)(x, y, self.xmean, self.ymean, sigma, 1)\n",
    "        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "        kernel = self.generate_kernel()\n",
    "\n",
    "        ## Add the batch dim if the input is a single element\n",
    "        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]\n",
    "        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n",
    "               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n",
    "               (self.strides, self.strides),\n",
    "               self.padding)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_function(x, y, xmean, ymean, sigma, A=1):\n",
    "        return A*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))\n",
    "\n",
    "    def return_kernel(self, params):\n",
    "        x, y = jnp.meshgrid(jnp.linspace(0,1,num=self.kernel_size), jnp.linspace(0,1,num=self.kernel_size))\n",
    "        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,None), out_axes=-1)(x, y, self.xmean, self.ymean, params[\"params\"][\"sigma\"], 1)\n",
    "        kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))\n",
    "        return kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GaussianLayer(nn.Module):\n",
    "    \"\"\"Parametric gaussian layer.\"\"\"\n",
    "    features: int\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    feature_group_count: int = 1\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "    xmean: float = 0.5\n",
    "    ymean: float = 0.5\n",
    "    fs: float = 1 # Sampling frequency\n",
    "    normalize_prob: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        is_initialized = self.has_variable(\"precalc_filter\", \"kernel\")\n",
    "        precalc_filters = self.variable(\"precalc_filter\",\n",
    "                                        \"kernel\",\n",
    "                                        jnp.zeros,\n",
    "                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "        sigma = self.param(\"sigma\",\n",
    "                           nn.initializers.uniform(scale=self.xmean),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        A = self.param(\"A\",\n",
    "                       nn.initializers.ones,\n",
    "                       (self.features*inputs.shape[-1],))\n",
    "\n",
    "        if is_initialized and not train: \n",
    "            kernel = precalc_filters.value\n",
    "        elif is_initialized and train: \n",
    "            x, y = self.generate_dominion()\n",
    "            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, A, self.normalize_prob)\n",
    "            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "            kernel = rearrange(kernel, \"(c_in c_out) kx ky -> kx ky c_in c_out\", c_in=inputs.shape[-1], c_out=self.features)\n",
    "            precalc_filters.value = kernel\n",
    "        else:\n",
    "            kernel = precalc_filters.value\n",
    "\n",
    "        ## Add the batch dim if the input is a single element\n",
    "        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False\n",
    "        else: had_batch = True\n",
    "        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n",
    "               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n",
    "               (self.strides, self.strides),\n",
    "               self.padding)\n",
    "        ## Move the channels back to the last dim\n",
    "        outputs = jnp.transpose(outputs, (0,2,3,1))\n",
    "        if not had_batch: outputs = outputs[0]\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):\n",
    "        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.\n",
    "        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma), 1.)\n",
    "        return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))\n",
    "\n",
    "    def return_kernel(self, params, c_in):\n",
    "        x, y = self.generate_dominion()\n",
    "        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None), out_axes=0)(x, y, self.xmean, self.ymean, params[\"params\"][\"sigma\"], params[\"params\"][\"A\"], self.normalize_prob)\n",
    "        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))\n",
    "        kernel = rearrange(kernel, \"(c_in c_out) kx ky -> kx ky c_in c_out\", c_in=c_in, c_out=self.features)\n",
    "        return kernel\n",
    "    \n",
    "    def generate_dominion(self):\n",
    "        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class GaussianLayerLogSigma(nn.Module):\n",
    "    \"\"\"Parametric gaussian layer that optimizes log(sigma) instead of sigma.\"\"\"\n",
    "    features: int\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    feature_group_count: int = 1\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "    xmean: float = 0.5\n",
    "    ymean: float = 0.5\n",
    "    fs: float = 1 # Sampling frequency\n",
    "    normalize_prob: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        is_initialized = self.has_variable(\"precalc_filter\", \"kernel\")\n",
    "        precalc_filters = self.variable(\"precalc_filter\",\n",
    "                                        \"kernel\",\n",
    "                                        jnp.zeros,\n",
    "                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "        logsigma = self.param(\"logsigma\",\n",
    "                           bounded_uniform(minval=-4., maxval=-0.5),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        A = self.param(\"A\",\n",
    "                       nn.initializers.ones,\n",
    "                       (self.features*inputs.shape[-1],))\n",
    "        sigma = jnp.exp(logsigma)\n",
    "\n",
    "        if is_initialized and not train: \n",
    "            kernel = precalc_filters.value\n",
    "        elif is_initialized and train: \n",
    "            x, y = self.generate_dominion()\n",
    "            kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None), out_axes=0)(x, y, self.xmean, self.ymean, sigma, A, self.normalize_prob)\n",
    "            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "            kernel = rearrange(kernel, \"(c_in c_out) kx ky -> kx ky c_in c_out\", c_in=inputs.shape[-1], c_out=self.features)\n",
    "            precalc_filters.value = kernel\n",
    "        else:\n",
    "            kernel = precalc_filters.value\n",
    "\n",
    "        ## Add the batch dim if the input is a single element\n",
    "        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False\n",
    "        else: had_batch = True\n",
    "        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n",
    "               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n",
    "               (self.strides, self.strides),\n",
    "               self.padding)\n",
    "        ## Move the channels back to the last dim\n",
    "        outputs = jnp.transpose(outputs, (0,2,3,1))\n",
    "        if not had_batch: outputs = outputs[0]\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(x, y, xmean, ymean, sigma, A=1, normalize_prob=True):\n",
    "        # A_norm = 1/(2*jnp.pi*sigma) if normalize_prob else 1.\n",
    "        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*sigma), 1.)\n",
    "        return A*A_norm*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))\n",
    "\n",
    "    def return_kernel(self, params, c_in):\n",
    "        x, y = self.generate_dominion()\n",
    "        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,0,None), out_axes=0)(x, y, self.xmean, self.ymean, jnp.exp(params[\"params\"][\"logsigma\"]), params[\"params\"][\"A\"], self.normalize_prob)\n",
    "        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))\n",
    "        kernel = rearrange(kernel, \"(c_in c_out) kx ky -> kx ky c_in c_out\", c_in=c_in, c_out=self.features)\n",
    "        return kernel\n",
    "    \n",
    "    def generate_dominion(self):\n",
    "        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter shapes:  FrozenDict({\n",
      "    A: (3,),\n",
      "    sigma: (3,),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.normal(key1, shape=(28,28,3))\n",
    "model = GaussianLayer(features=1, kernel_size=21, fs=21)\n",
    "variables = model.init(key2, x)\n",
    "state, params = variables.pop(\"params\")\n",
    "print(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import matplotlib.pyplot as plt\n",
    "kernel = model.return_kernel({\"params\": params}, c_in=3)\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "for k, sigma, ax in zip(rearrange(kernel, \"kx ky cin cout -> (cin cout) kx ky\"), params[\"sigma\"], axes):\n",
    "    ax.imshow(k)\n",
    "    ax.set_title(sigma)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test if the precalculated filters are updated when in training mode and stay the same when in evaluation mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 154 ms, sys: 7.1 ms, total: 161 ms\n",
      "Wall time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=True)\n",
    "assert not jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 ms, sys: 1.11 ms, total: 19.4 ms\n",
      "Wall time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=False)\n",
    "assert jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a time difference in both executions, which makes sense because when `train=False`, the filters don't have to be calculated and thus the function should run faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if we're able to train the layer weights while maintaining the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1))\n",
    "def update_step(apply_fn, tx, inputs, opt_state, params, state):\n",
    "    def loss(params):\n",
    "        pred, updated_state = apply_fn({\"params\": params, **state}, \n",
    "                                       x, \n",
    "                                       mutable=list(state.keys()), \n",
    "                                       train=True)\n",
    "        loss = ((pred-inputs)**2).mean()\n",
    "        return loss, updated_state\n",
    "    (l, updated_state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return opt_state, params, updated_state, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0: 46.98198318481445\n",
      "Loss 100: 16.03009033203125\n",
      "Loss 200: 8.65085506439209\n",
      "Loss 300: 5.960490703582764\n",
      "Loss 400: 4.6348466873168945\n",
      "Loss 500: 3.844392776489258\n",
      "Loss 600: 3.3134639263153076\n",
      "Loss 700: 2.9298505783081055\n",
      "Loss 800: 2.639446973800659\n",
      "Loss 900: 2.4124348163604736\n",
      "Loss 1000: 2.2307052612304688\n"
     ]
    }
   ],
   "source": [
    "model = GaussianLayer(features=3, kernel_size=21, fs=21)\n",
    "variables = model.init(random.PRNGKey(0), x)\n",
    "# Split state and params (which are updated by optimizer).\n",
    "state, params = variables.pop('params')\n",
    "del variables  # Delete variables to avoid wasting resources\n",
    "tx = optax.sgd(learning_rate=3e-4)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "for i in range(1001):\n",
    "  opt_state, params, state, loss = update_step(\n",
    "      model.apply, tx, x, opt_state, params, state)\n",
    "  if i % 100 == 0: print(f\"Loss {i}: {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is going down, so everything looking good so far!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor layer\n",
    "\n",
    "> We'll repeat the process but now with a Gabor functional form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GaborLayer(nn.Module):\n",
    "    \"\"\"Parametric Gabor layer.\"\"\"\n",
    "    features: int\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    feature_group_count: int = 1\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "    xmean: float = 0.5\n",
    "    ymean: float = 0.5\n",
    "    fs: float = 1 # Sampling frequency\n",
    "\n",
    "    normalize_prob: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        is_initialized = self.has_variable(\"precalc_filter\", \"kernel\")\n",
    "        precalc_filters = self.variable(\"precalc_filter\",\n",
    "                                        \"kernel\",\n",
    "                                        jnp.zeros,\n",
    "                                        (self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "        freq = self.param(\"freq\",\n",
    "                           nn.initializers.uniform(scale=self.fs/2),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        logsigmax = self.param(\"logsigmax\",\n",
    "                           bounded_uniform(minval=-4., maxval=-0.5),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        logsigmay = self.param(\"logsigmay\",\n",
    "                           bounded_uniform(minval=-4., maxval=-0.5),\n",
    "                           (self.features*inputs.shape[-1],))        \n",
    "        theta = self.param(\"theta\",\n",
    "                           nn.initializers.uniform(scale=jnp.pi),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        sigma_theta = self.param(\"sigma_theta\",\n",
    "                           nn.initializers.uniform(scale=jnp.pi),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        rot_theta = self.param(\"rot_theta\",\n",
    "                           nn.initializers.uniform(scale=jnp.pi),\n",
    "                           (self.features*inputs.shape[-1],))\n",
    "        A = self.param(\"A\",\n",
    "                       nn.initializers.ones,\n",
    "                       (self.features*inputs.shape[-1],))\n",
    "        sigmax, sigmay = jnp.exp(logsigmax), jnp.exp(logsigmay)\n",
    "\n",
    "        if is_initialized and not train: \n",
    "            kernel = precalc_filters.value\n",
    "        elif is_initialized and train: \n",
    "            x, y = self.generate_dominion()\n",
    "            # gabor_fn = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,None,None))\n",
    "            kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,0,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, A, self.normalize_prob)\n",
    "            kernel = rearrange(kernel, \"(c_in c_out) kx ky -> kx ky c_in c_out\", c_in=inputs.shape[-1], c_out=self.features)\n",
    "            # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n",
    "            precalc_filters.value = kernel\n",
    "        else:\n",
    "            kernel = precalc_filters.value\n",
    "\n",
    "        ## Add the batch dim if the input is a single element\n",
    "        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False\n",
    "        else: had_batch = True\n",
    "        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n",
    "               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n",
    "               (self.strides, self.strides),\n",
    "               self.padding)\n",
    "        ## Move the channels back to the last dim\n",
    "        outputs = jnp.transpose(outputs, (0,2,3,1))\n",
    "        if not had_batch: outputs = outputs[0]\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def gabor(x, y, xmean, ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, A=1, normalize_prob=True):\n",
    "        # ## Rotate the dominion\n",
    "        # x = jnp.cos(rot_theta) * (x - xmean) - jnp.sin(rot_theta) * (y - ymean)\n",
    "        # y = jnp.sin(rot_theta) * (x - xmean) + jnp.cos(rot_theta) * (y - ymean)\n",
    "        x, y = x-xmean, y-ymean\n",
    "        ## Obtain the normalization coeficient\n",
    "        sigma_vector = jnp.array([sigmax, sigmay])\n",
    "        cov_matrix = jnp.diag(sigma_vector)**2\n",
    "        det_cov_matrix = jnp.linalg.det(cov_matrix)\n",
    "        # A_norm = 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)) if normalize_prob else 1.\n",
    "        A_norm = jnp.where(normalize_prob, 1/(2*jnp.pi*jnp.sqrt(det_cov_matrix)), 1.)\n",
    "        \n",
    "        ## Rotate the sinusoid\n",
    "        rotation_matrix = jnp.array([[jnp.cos(sigma_theta), -jnp.sin(sigma_theta)],\n",
    "                                     [jnp.sin(sigma_theta), jnp.cos(sigma_theta)]])\n",
    "        rotated_covariance = rotation_matrix @ jnp.linalg.inv(cov_matrix) @ jnp.transpose(rotation_matrix)\n",
    "        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y\n",
    "        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y\n",
    "        distance = x * x_r_1 + y * y_r_1\n",
    "\n",
    "        return A*A_norm*jnp.exp(-distance/2) * jnp.cos(2*jnp.pi*freq*(x*jnp.cos(theta)+y*jnp.sin(theta)))\n",
    "\n",
    "    def return_kernel(self, params, c_in=3):\n",
    "        x, y = self.generate_dominion()\n",
    "        sigmax, sigmay = jnp.exp(params[\"logsigmax\"]), jnp.exp(params[\"logsigmay\"])\n",
    "        # sigmax, sigmay = jnp.exp(params[\"sigmax\"]), jnp.exp(params[\"sigmay\"])\n",
    "        kernel = jax.vmap(self.gabor, in_axes=(None,None,None,None,0,0,0,0,0,0,0,None), out_axes=0)(x, y, self.xmean, self.ymean, sigmax, sigmay, params[\"freq\"], params[\"theta\"], params[\"sigma_theta\"], params[\"rot_theta\"], params[\"A\"], self.normalize_prob)\n",
    "        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, input_channels, self.features))\n",
    "        kernel = rearrange(kernel, \"(c_in c_out) kx ky -> kx ky c_in c_out\", c_in=c_in, c_out=self.features)\n",
    "        return kernel\n",
    "    \n",
    "    def generate_dominion(self):\n",
    "        return jnp.meshgrid(jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1], jnp.linspace(0,self.kernel_size/self.fs,num=self.kernel_size+1)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter shapes:  FrozenDict({\n",
      "    A: (3,),\n",
      "    freq: (3,),\n",
      "    logsigmax: (3,),\n",
      "    logsigmay: (3,),\n",
      "    rot_theta: (3,),\n",
      "    sigma_theta: (3,),\n",
      "    theta: (3,),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.normal(key1, shape=(28,28,3))\n",
    "model = GaborLayer(features=1, kernel_size=21, fs=21)\n",
    "variables = model.init(key2, x)\n",
    "state, params = variables.pop(\"params\")\n",
    "print(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import matplotlib.pyplot as plt\n",
    "kernel = model.return_kernel(params, c_in=3)\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "# for k, sigmax, sigmay, ax in zip(rearrange(kernel, \"kx ky cin cout -> (cin cout) kx ky\"), params[\"sigmax\"], params[\"sigmay\"], axes):\n",
    "for k, sigmax, sigmay, ax in zip(rearrange(kernel, \"kx ky cin cout -> (cin cout) kx ky\"), jnp.exp(params[\"logsigmax\"]), jnp.exp(params[\"logsigmay\"]), axes):\n",
    "    ax.imshow(k)\n",
    "    ax.set_title(f\"{sigmax:.2f}, {sigmay:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test if the precalculated filters are updated when in training mode and stay the same when in evaluation mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.5 ms, sys: 20.9 ms, total: 92.4 ms\n",
      "Wall time: 78.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=True)\n",
    "assert not jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 16.3 ms, total: 133 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=False)\n",
    "assert jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a time difference in both executions, which makes sense because when `train=False`, the filters don't have to be calculated and thus the function should run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if we're able to train the layer weights while maintaining the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1))\n",
    "def update_step(apply_fn, tx, inputs, opt_state, params, state):\n",
    "    def loss(params):\n",
    "        pred, updated_state = apply_fn({\"params\": params, **state}, \n",
    "                                       x, \n",
    "                                       mutable=list(state.keys()), \n",
    "                                       train=True)\n",
    "        loss = ((pred-inputs)**2).mean()\n",
    "        return loss, updated_state\n",
    "    (l, updated_state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return opt_state, params, updated_state, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    freq: Array([ 0.6816269, 10.3780775,  1.1226437], dtype=float32),\n",
       "    logsigmax: Array([-2.4631429 , -2.717532  , -0.77407956], dtype=float32),\n",
       "    logsigmay: Array([-3.0342553 , -2.29462   , -0.74401665], dtype=float32),\n",
       "    theta: Array([3.050746 , 0.9813687, 1.7434841], dtype=float32),\n",
       "    sigma_theta: Array([2.1122246, 2.7645009, 0.9459793], dtype=float32),\n",
       "    rot_theta: Array([2.5787003, 1.4155393, 0.6662881], dtype=float32),\n",
       "    A: Array([1., 1., 1.], dtype=float32),\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaborLayer(features=1, kernel_size=21, fs=21)\n",
    "variables = model.init(random.PRNGKey(0), x)\n",
    "# Split state and params (which are updated by optimizer).\n",
    "state, params = variables.pop('params')\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    freq: Array([ 0.6816269, 10.3780775,  1.1226437], dtype=float32),\n",
      "    logsigmax: Array([-2.4631429 , -2.717532  , -0.77407956], dtype=float32),\n",
      "    logsigmay: Array([-3.0342553 , -2.29462   , -0.74401665], dtype=float32),\n",
      "    theta: Array([3.050746 , 0.9813687, 1.7434841], dtype=float32),\n",
      "    sigma_theta: Array([2.1122246, 2.7645009, 0.9459793], dtype=float32),\n",
      "    rot_theta: Array([2.5787003, 1.4155393, 0.6662881], dtype=float32),\n",
      "    A: Array([1., 1., 1.], dtype=float32),\n",
      "})\n",
      "Loss 0: 11759.92578125\n",
      "Loss 100: 6.003411769866943\n",
      "Loss 200: 3.3821327686309814\n",
      "Loss 300: 2.540633201599121\n",
      "Loss 400: 2.1371352672576904\n",
      "Loss 500: 1.9039528369903564\n",
      "Loss 600: 1.7531646490097046\n",
      "Loss 700: 1.6479045152664185\n",
      "Loss 800: 1.5702329874038696\n",
      "Loss 900: 1.5104745626449585\n",
      "Loss 1000: 1.4629968404769897\n"
     ]
    }
   ],
   "source": [
    "model = GaborLayer(features=1, kernel_size=21, fs=21)\n",
    "variables = model.init(random.PRNGKey(0), x)\n",
    "# Split state and params (which are updated by optimizer).\n",
    "state, params = variables.pop('params')\n",
    "del variables  # Delete variables to avoid wasting resources\n",
    "tx = optax.sgd(learning_rate=3e-4)\n",
    "opt_state = tx.init(params)\n",
    "print(params)\n",
    "\n",
    "for i in range(1001):\n",
    "  opt_state, params, state, loss = update_step(\n",
    "      model.apply, tx, x, opt_state, params, state)\n",
    "  if i % 100 == 0: print(f\"Loss {i}: {loss}\")\n",
    "  # print(f\"Loss {i}: {loss}\")\n",
    "  # print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is going down, so everything looking good so far!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
