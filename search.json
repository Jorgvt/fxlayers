[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fxlayers",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "fxlayers",
    "section": "Install",
    "text": "Install\npip install fxlayers"
  },
  {
    "objectID": "initializers.html",
    "href": "initializers.html",
    "title": "Initializers",
    "section": "",
    "text": "I found it weird that the flax library didn’t provide a “bounded” uniform initializer, so I took the code for the uniform one and modified it in a way that it allowed setting the bounds of the desired uniform initialization.\n\nsource\n\nbounded_uniform\n\n bounded_uniform (minval=0.0, maxval=1.0, dtype=&lt;class 'numpy.float64'&gt;)\n\n\nlayer = nn.Dense(features=1000, kernel_init=bounded_uniform(minval=-10., maxval=-5.))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,32,32,3)))\nplt.hist(params[\"params\"][\"kernel\"].ravel())\nplt.show()\n\n2023-10-26 11:19:39.543311: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\n\n\nlayer = nn.Dense(features=1000, kernel_init=bounded_uniform(minval=-10., maxval=25.))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,32,32,3)))\nplt.hist(params[\"params\"][\"kernel\"].ravel())\nplt.show()\n\n\n\n\n\nsource\n\n\ndisplaced_normal\n\n displaced_normal (mean=0.0, stddev=0.01, dtype=&lt;class 'numpy.float64'&gt;)\n\nBuilds an initializer that returns real normally-distributed random arrays.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmean\nfloat\n0.0\nMean of the distribution.\n\n\nstddev\nfloat\n0.01\nStandard deviation of the distribution.\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Dense(features=1000, kernel_init=displaced_normal())\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,32,32,3)))\nplt.hist(params[\"params\"][\"kernel\"].ravel())\nplt.show()\n\n\n\n\n\nlayer = nn.Dense(features=1000, kernel_init=displaced_normal(mean=5., stddev=2.))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,32,32,3)))\nplt.hist(params[\"params\"][\"kernel\"].ravel())\nplt.show()\n\n\n\n\n\nsource\n\n\nfreq_scales_init\n\n freq_scales_init (n_scales, fs, dtype=&lt;class 'numpy.float64'&gt;)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_scales\n\n\nNumber of scales.\n\n\nfs\n\n\nSampling frequency.\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Dense(features=4, kernel_init=freq_scales_init(n_scales=4, fs=64))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,4)))\nparams\n\nFrozenDict({\n    params: {\n        kernel: Array([24., 12.,  6.,  3.], dtype=float32),\n        bias: Array([0., 0., 0., 0.], dtype=float32),\n    },\n})\n\n\n\nsource\n\n\nk_array\n\n k_array (k, arr, dtype=&lt;class 'numpy.float64'&gt;)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\n\n\nNumber of scales.\n\n\narr\n\n\nSampling frequency.\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Dense(features=4, kernel_init=k_array(k=2, arr=jnp.array([24., 12., 6., 3.])))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,4)))\nparams\n\nFrozenDict({\n    params: {\n        kernel: Array([0.08333334, 0.16666667, 0.33333334, 0.6666667 ], dtype=float32),\n        bias: Array([0., 0., 0., 0.], dtype=float32),\n    },\n})\n\n\n\nsource\n\n\nlog_k_array\n\n log_k_array (k, arr, dtype=&lt;class 'numpy.float64'&gt;)\n\nInitializer that generates the weights based on applying the log to a given array.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\n\n\nNumber of scales.\n\n\narr\n\n\nSampling frequency.\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Dense(features=4, kernel_init=log_k_array(k=1, arr=1/jnp.array([24., 12., 6., 3.])**2))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,4)))\nparams\n\nFrozenDict({\n    params: {\n        kernel: Array([-6.3561077, -4.9698133, -3.583519 , -2.1972246], dtype=float32),\n        bias: Array([0., 0., 0., 0.], dtype=float32),\n    },\n})\n\n\n\nsource\n\n\nlinspace\n\n linspace (start, stop, num, dtype=&lt;class 'numpy.float64'&gt;)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\n\n\n\n\n\nstop\n\n\n\n\n\nnum\n\n\n\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Dense(features=4, kernel_init=linspace(start=0, stop=jnp.pi, num=4))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,4)))\nparams\n\nFrozenDict({\n    params: {\n        kernel: Array([0.       , 0.7853982, 1.5707964, 2.3561945], dtype=float32),\n        bias: Array([0., 0., 0., 0.], dtype=float32),\n    },\n})\n\n\n\nsource\n\n\nequal_to\n\n equal_to (arr, dtype=&lt;class 'numpy.float64'&gt;)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\narr\n\n\n\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Dense(features=4, kernel_init=equal_to([1., 2., 3., 4.]))\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,4)))\nparams\n\nFrozenDict({\n    params: {\n        kernel: Array([1., 2., 3., 4.], dtype=float32),\n        bias: Array([0., 0., 0., 0.], dtype=float32),\n    },\n})\n\n\n\nsource\n\n\nmean\n\n mean (dtype=&lt;class 'numpy.float64'&gt;)\n\nBuilds an initializer that returns a kernel that calculates the mean of the interacting pixels.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndtype\ntype\nfloat64\nDesired DType of the resulting array.\n\n\n\n\nlayer = nn.Conv(features=1, kernel_size=(1,1), kernel_init=mean())\nparams = layer.init(random.PRNGKey(0), jnp.ones(shape=(1,32,32,3)))\nplt.hist(params[\"params\"][\"kernel\"].ravel())\nplt.show()"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Functional layers",
    "section": "",
    "text": "First we’ll define a base class that will be used by every other functional layer.\n\n\nclass BaseFunctional(nn.Module):\n    \"\"\"Base functional layer.\"\"\"\n    features: int\n    kernel_size: Union[int, Sequence[int]]\n    strides: int = 1\n    padding: str = \"SAME\"\n    feature_group_count: int = 1\n    kernel_init: Callable = nn.initializers.lecun_normal()\n    bias_init: Callable = nn.initializers.zeros_init()\n    xmean: float = 0.5\n    ymean: float = 0.5\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        sigma = self.param(\"sigma\",\n                           nn.initializers.uniform(scale=1),\n                           (self.features*inputs.shape[-1],))\n\n        # x, y = jnp.meshgrid(jnp.linspace(0,1,num=self.kernel_size), jnp.linspace(0,1,num=self.kernel_size))\n        # kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,None), out_axes=-1)(x, y, self.xmean, self.ymean, sigma, 1)\n        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n        kernel = self.generate_kernel()\n\n        ## Add the batch dim if the input is a single element\n        if jnp.ndim(inputs) &lt; 4: inputs = inputs[None,:]\n        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n               (self.strides, self.strides),\n               self.padding)\n        return outputs\n\n    @staticmethod\n    def generate_function(x, y, xmean, ymean, sigma, A=1):\n        return A*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))\n\n    def return_kernel(self, params):\n        x, y = jnp.meshgrid(jnp.linspace(0,1,num=self.kernel_size), jnp.linspace(0,1,num=self.kernel_size))\n        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,None), out_axes=-1)(x, y, self.xmean, self.ymean, params[\"params\"][\"sigma\"], 1)\n        kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))\n        return kernel"
  },
  {
    "objectID": "layers.html#base-layer",
    "href": "layers.html#base-layer",
    "title": "Functional layers",
    "section": "",
    "text": "First we’ll define a base class that will be used by every other functional layer.\n\n\nclass BaseFunctional(nn.Module):\n    \"\"\"Base functional layer.\"\"\"\n    features: int\n    kernel_size: Union[int, Sequence[int]]\n    strides: int = 1\n    padding: str = \"SAME\"\n    feature_group_count: int = 1\n    kernel_init: Callable = nn.initializers.lecun_normal()\n    bias_init: Callable = nn.initializers.zeros_init()\n    xmean: float = 0.5\n    ymean: float = 0.5\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        sigma = self.param(\"sigma\",\n                           nn.initializers.uniform(scale=1),\n                           (self.features*inputs.shape[-1],))\n\n        # x, y = jnp.meshgrid(jnp.linspace(0,1,num=self.kernel_size), jnp.linspace(0,1,num=self.kernel_size))\n        # kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,None), out_axes=-1)(x, y, self.xmean, self.ymean, sigma, 1)\n        # kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, inputs.shape[-1], self.features))\n        kernel = self.generate_kernel()\n\n        ## Add the batch dim if the input is a single element\n        if jnp.ndim(inputs) &lt; 4: inputs = inputs[None,:]\n        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n               jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n               (self.strides, self.strides),\n               self.padding)\n        return outputs\n\n    @staticmethod\n    def generate_function(x, y, xmean, ymean, sigma, A=1):\n        return A*jnp.exp(-((x-xmean)**2 + (y-ymean)**2)/(2*sigma**2))\n\n    def return_kernel(self, params):\n        x, y = jnp.meshgrid(jnp.linspace(0,1,num=self.kernel_size), jnp.linspace(0,1,num=self.kernel_size))\n        kernel = jax.vmap(self.gaussian, in_axes=(None,None,None,None,0,None), out_axes=-1)(x, y, self.xmean, self.ymean, params[\"params\"][\"sigma\"], 1)\n        kernel = jnp.reshape(kernel, newshape=(self.kernel_size, self.kernel_size, 3, self.features))\n        return kernel"
  },
  {
    "objectID": "layers.html#gaussian-layer",
    "href": "layers.html#gaussian-layer",
    "title": "Functional layers",
    "section": "Gaussian layer",
    "text": "Gaussian layer\n\nsource\n\nGaussianLayer\n\n GaussianLayer (features:int, kernel_size:Union[int,Sequence[int]],\n                strides:int=1, padding:str='SAME',\n                feature_group_count:int=1, kernel_init:Callable=&lt;function\n                init&gt;, bias_init:Callable=&lt;function zeros&gt;,\n                use_bias:bool=False, xmean:float=0.5, ymean:float=0.5,\n                fs:float=1, normalize_prob:bool=True,\n                normalize_energy:bool=False, parent:Union[Type[flax.linen.\n                module.Module],Type[flax.core.scope.Scope],Type[flax.linen\n                .module._Sentinel],NoneType]=&lt;flax.linen.module._Sentinel\n                object at 0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nParametric gaussian layer.\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = GaussianLayer(features=1, kernel_size=5, fs=5)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\n2023-10-26 11:00:50.291617: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nParameter shapes:  FrozenDict({\n    A: (3,),\n    sigma: (3,),\n})\n\n\n\nimport matplotlib.pyplot as plt\nkernel = model.return_kernel({\"params\": params}, c_in=3)\nfig, axes = plt.subplots(1, 3)\nfor k, sigma, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), params[\"sigma\"], axes):\n    ax.imshow(k)\n    ax.set_title(sigma)\nplt.show()\n\n\n\n\nWe can also apply a grouped convolution:\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = GaussianLayer(features=3, kernel_size=5, fs=5, feature_group_count=3)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    A: (3,),\n    sigma: (3,),\n})\n\n\n\nimport matplotlib.pyplot as plt\nkernel = model.return_kernel({\"params\": params}, c_in=3)\nfig, axes = plt.subplots(1, 3)\nfor k, sigma, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), params[\"sigma\"], axes):\n    ax.imshow(k)\n    ax.set_title(sigma)\nplt.show()\n\n\n\n\nWe can test if the precalculated filters are updated when in training mode and stay the same when in evaluation mode:\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=True)\nassert not jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]\n\nCPU times: user 54.5 ms, sys: 0 ns, total: 54.5 ms\nWall time: 53.4 ms\n\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=False)\nassert jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]\n\nCPU times: user 8.35 ms, sys: 204 µs, total: 8.56 ms\nWall time: 7.99 ms\n\n\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = GaussianLayerGamma(features=1, kernel_size=5, fs=5)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    A: (3,),\n    gamma: (3,),\n})\n\n\n\nimport matplotlib.pyplot as plt\nkernel = model.return_kernel({\"params\": params}, c_in=3)\nfig, axes = plt.subplots(1, 3)\nfor k, sigma, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), 1/params[\"gamma\"], axes):\n    ax.imshow(k)\n    ax.set_title(sigma)\nplt.show()\n\n\n\n\nWe see that there is a time difference in both executions, which makes sense because when train=False, the filters don’t have to be calculated and thus the function should run faster.\nAnd let’s see if we’re able to train the layer weights while maintaining the state:\n\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(0, 1))\ndef update_step(apply_fn, tx, inputs, opt_state, params, state):\n    def loss(params):\n        pred, updated_state = apply_fn({\"params\": params, **state}, \n                                       x, \n                                       mutable=list(state.keys()), \n                                       train=True)\n        loss = ((pred-inputs)**2).mean()\n        return loss, updated_state\n    (l, updated_state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n    updates, opt_state = tx.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return opt_state, params, updated_state, l\n\n\nmodel = GaussianLayer(features=3, kernel_size=21, fs=21)\nvariables = model.init(random.PRNGKey(0), x)\n# Split state and params (which are updated by optimizer).\nstate, params = variables.pop('params')\ndel variables  # Delete variables to avoid wasting resources\ntx = optax.sgd(learning_rate=3e-4)\nopt_state = tx.init(params)\n\nfor i in range(1001):\n  opt_state, params, state, loss = update_step(\n      model.apply, tx, x, opt_state, params, state)\n  if i % 100 == 0: print(f\"Loss {i}: {loss}\")\n\nLoss 0: 63022.55078125\nLoss 100: 6.95550537109375\nLoss 200: 4.45768928527832\nLoss 300: 3.467116117477417\nLoss 400: 2.92319393157959\nLoss 500: 2.5758800506591797\nLoss 600: 2.3336243629455566\nLoss 700: 2.154496431350708\nLoss 800: 2.016449451446533\nLoss 900: 1.9067078828811646\nLoss 1000: 1.8173458576202393\n\n\nThe loss is going down, so everything looking good so far!"
  },
  {
    "objectID": "layers.html#gabor-layer",
    "href": "layers.html#gabor-layer",
    "title": "Functional layers",
    "section": "Gabor layer",
    "text": "Gabor layer\n\nWe’ll repeat the process but now with a Gabor functional form.\n\n\nsource\n\nGaborLayer\n\n GaborLayer (features:int, kernel_size:Union[int,Sequence[int]],\n             strides:int=1, padding:str='SAME', feature_group_count:int=1,\n             kernel_init:Callable=&lt;function init&gt;,\n             bias_init:Callable=&lt;function zeros&gt;, use_bias:bool=False,\n             xmean:float=0.5, ymean:float=0.5, fs:float=1,\n             normalize_prob:bool=True, normalize_energy:bool=False, parent\n             :Union[Type[flax.linen.module.Module],Type[flax.core.scope.Sc\n             ope],Type[flax.linen.module._Sentinel],NoneType]=&lt;flax.linen.\n             module._Sentinel object at 0x7f1c5f19dcd0&gt;,\n             name:Optional[str]=None)\n\nParametric Gabor layer.\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = GaborLayer(features=1, kernel_size=21, fs=21)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    A: (3,),\n    freq: (3,),\n    logsigmax: (3,),\n    logsigmay: (3,),\n    rot_theta: (3,),\n    sigma_theta: (3,),\n    theta: (3,),\n})\n\n\n\nimport matplotlib.pyplot as plt\nkernel = model.return_kernel(params, c_in=3)\nfig, axes = plt.subplots(1, 3)\n# for k, sigmax, sigmay, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), params[\"sigmax\"], params[\"sigmay\"], axes):\nfor k, sigmax, sigmay, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), jnp.exp(params[\"logsigmax\"]), jnp.exp(params[\"logsigmay\"]), axes):\n    ax.imshow(k)\n    ax.set_title(f\"{sigmax:.2f}, {sigmay:.2f}\")\nplt.show()\n\n\n\n\nWe can test if the precalculated filters are updated when in training mode and stay the same when in evaluation mode:\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=True)\nassert not jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]\n\nCPU times: user 126 ms, sys: 0 ns, total: 126 ms\nWall time: 93.3 ms\n\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=False)\nassert jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]\n\nCPU times: user 53.7 ms, sys: 848 µs, total: 54.6 ms\nWall time: 21.3 ms\n\n\nWe see that there is a time difference in both executions, which makes sense because when train=False, the filters don’t have to be calculated and thus the function should run faster.\nAnd let’s see if we’re able to train the layer weights while maintaining the state:\n\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(0, 1))\ndef update_step(apply_fn, tx, inputs, opt_state, params, state):\n    def loss(params):\n        pred, updated_state = apply_fn({\"params\": params, **state}, \n                                       x, \n                                       mutable=list(state.keys()), \n                                       train=True)\n        loss = ((pred-inputs)**2).mean()\n        return loss, updated_state\n    (l, updated_state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n    updates, opt_state = tx.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return opt_state, params, updated_state, l\n\n\nmodel = GaborLayer(features=1, kernel_size=21, fs=21)\nvariables = model.init(random.PRNGKey(0), x)\n# Split state and params (which are updated by optimizer).\nstate, params = variables.pop('params')\nparams\n\nFrozenDict({\n    freq: Array([ 0.6816269, 10.3780775,  1.1226437], dtype=float32),\n    logsigmax: Array([-2.4631429, -2.717532 , -0.7740796], dtype=float32),\n    logsigmay: Array([-3.0342553 , -2.29462   , -0.74401677], dtype=float32),\n    theta: Array([3.050746 , 0.9813687, 1.7434841], dtype=float32),\n    sigma_theta: Array([2.1122246, 2.7645009, 0.9459793], dtype=float32),\n    rot_theta: Array([2.5787003, 1.4155393, 0.6662881], dtype=float32),\n    A: Array([1., 1., 1.], dtype=float32),\n})\n\n\n\nmodel = GaborLayer(features=1, kernel_size=21, fs=21)\nvariables = model.init(random.PRNGKey(0), x)\n# Split state and params (which are updated by optimizer).\nstate, params = variables.pop('params')\ndel variables  # Delete variables to avoid wasting resources\ntx = optax.sgd(learning_rate=3e-4)\nopt_state = tx.init(params)\nprint(params)\n\nfor i in range(1001):\n  opt_state, params, state, loss = update_step(\n      model.apply, tx, x, opt_state, params, state)\n  if i % 100 == 0: print(f\"Loss {i}: {loss}\")\n  # print(f\"Loss {i}: {loss}\")\n  # print(params)\n\nFrozenDict({\n    freq: Array([ 0.6816269, 10.3780775,  1.1226437], dtype=float32),\n    logsigmax: Array([-2.4631429, -2.717532 , -0.7740796], dtype=float32),\n    logsigmay: Array([-3.0342553 , -2.29462   , -0.74401677], dtype=float32),\n    theta: Array([3.050746 , 0.9813687, 1.7434841], dtype=float32),\n    sigma_theta: Array([2.1122246, 2.7645009, 0.9459793], dtype=float32),\n    rot_theta: Array([2.5787003, 1.4155393, 0.6662881], dtype=float32),\n    A: Array([1., 1., 1.], dtype=float32),\n})\nLoss 0: 10626.9873046875\nLoss 100: 6.143118381500244\nLoss 200: 3.38569712638855\nLoss 300: 2.52960205078125\nLoss 400: 2.1249730587005615\nLoss 500: 1.892602801322937\nLoss 600: 1.7426190376281738\nLoss 700: 1.6378751993179321\nLoss 800: 1.560489296913147\nLoss 900: 1.50087308883667\nLoss 1000: 1.4534586668014526\n\n\nThe loss is going down, so everything looking good so far!"
  },
  {
    "objectID": "layers.html#center-surround-layer",
    "href": "layers.html#center-surround-layer",
    "title": "Functional layers",
    "section": "Center Surround layer",
    "text": "Center Surround layer\n\nsource\n\nCenterSurroundLogSigma\n\n CenterSurroundLogSigma (features:int,\n                         kernel_size:Union[int,Sequence[int]],\n                         strides:int=1, padding:str='SAME',\n                         feature_group_count:int=1,\n                         kernel_init:Callable=&lt;function init&gt;,\n                         bias_init:Callable=&lt;function zeros&gt;,\n                         use_bias:bool=False, xmean:float=0.5,\n                         ymean:float=0.5, fs:float=1,\n                         normalize_prob:bool=True,\n                         normalize_energy:bool=False, parent:Union[Type[fl\n                         ax.linen.module.Module],Type[flax.core.scope.Scop\n                         e],Type[flax.linen.module._Sentinel],NoneType]=&lt;f\n                         lax.linen.module._Sentinel object at\n                         0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nParametric center surround layer that optimizes log(sigma) instead of sigma.\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = CenterSurroundLogSigma(features=1, kernel_size=21, fs=21)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    A: (3,),\n    logsigma: (3,),\n    logsigma2: (3,),\n})\n\n\n\nimport matplotlib.pyplot as plt\nkernel = model.return_kernel({\"params\": params}, c_in=3)\nfig, axes = plt.subplots(1, 3)\nfor k, sigma, sigma2, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), jnp.exp(params[\"logsigma\"]), jnp.exp(params[\"logsigma2\"]), axes):\n    ax.imshow(k)\n    ax.set_title(f\"{sigma:.2f} | {sigma2:.2f}\")\nplt.show()\n\n\n\n\n\nsource\n\n\nCenterSurroundLogSigmaK\n\n CenterSurroundLogSigmaK (features:int,\n                          kernel_size:Union[int,Sequence[int]],\n                          strides:int=1, padding:str='SAME',\n                          feature_group_count:int=1,\n                          kernel_init:Callable=&lt;function init&gt;,\n                          bias_init:Callable=&lt;function zeros&gt;,\n                          use_bias:bool=False, xmean:float=0.5,\n                          ymean:float=0.5, fs:float=1,\n                          normalize_prob:bool=True,\n                          normalize_energy:bool=True, parent:Union[Type[fl\n                          ax.linen.module.Module],Type[flax.core.scope.Sco\n                          pe],Type[flax.linen.module._Sentinel],NoneType]=\n                          &lt;flax.linen.module._Sentinel object at\n                          0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nParametric center surround layer that optimizes log(sigma) instead of sigma and has a factor K instead of a second sigma.\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = CenterSurroundLogSigmaK(features=1, kernel_size=21, fs=21)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    A: (3,),\n    K: (3,),\n    logsigma: (3,),\n})\n\n\n\nimport matplotlib.pyplot as plt\nkernel = model.return_kernel({\"params\": params}, c_in=3)\nfig, axes = plt.subplots(1, 3)\nfor k, sigma, K, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), jnp.exp(params[\"logsigma\"]), params[\"K\"], axes):\n    ax.imshow(k)\n    ax.set_title(f\"{sigma:.2f} | {K:.2f}\")\nplt.show()\n\n\n\n\nWe can test if the precalculated filters are updated when in training mode and stay the same when in evaluation mode:\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=True)\nassert not jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]\n\nCPU times: user 69.3 ms, sys: 692 µs, total: 70 ms\nWall time: 44.5 ms\n\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, mutable=list(state.keys()), train=False)\nassert jax.tree_util.tree_map(lambda x,y: (x==y).all(), state, updated_state)[\"precalc_filter\"][\"kernel\"]\n\nCPU times: user 42.9 ms, sys: 3.31 ms, total: 46.2 ms\nWall time: 18.2 ms\n\n\nWe see that there is a time difference in both executions, which makes sense because when train=False, the filters don’t have to be calculated and thus the function should run faster.\nAnd let’s see if we’re able to train the layer weights while maintaining the state:\n\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(0, 1))\ndef update_step(apply_fn, tx, inputs, opt_state, params, state):\n    def loss(params):\n        pred, updated_state = apply_fn({\"params\": params, **state}, \n                                       x, \n                                       mutable=list(state.keys()), \n                                       train=True)\n        loss = ((pred-inputs)**2).mean()\n        return loss, updated_state\n    (l, updated_state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n    updates, opt_state = tx.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return opt_state, params, updated_state, l\n\n\nmodel = CenterSurroundLogSigmaK(features=3, kernel_size=21, fs=21)\nvariables = model.init(random.PRNGKey(0), x)\n# Split state and params (which are updated by optimizer).\nstate, params = variables.pop('params')\ndel variables  # Delete variables to avoid wasting resources\ntx = optax.sgd(learning_rate=3e-4)\nopt_state = tx.init(params)\n\nfor i in range(1001):\n  opt_state, params, state, loss = update_step(\n      model.apply, tx, x, opt_state, params, state)\n  if i % 100 == 0: print(f\"Loss {i}: {loss}\")\n\nLoss 0: 3.696951150894165\nLoss 100: 3.5763230323791504\nLoss 200: 3.461315870285034\nLoss 300: 3.6426374912261963\nLoss 400: 3.516855239868164\nLoss 500: 3.403351068496704\nLoss 600: 3.2899224758148193\nLoss 700: 3.182372808456421\nLoss 800: 3.0803799629211426\nLoss 900: 2.9836363792419434\nLoss 1000: 2.8918251991271973\n\n\nThe loss is going down, so everything looking good so far!"
  },
  {
    "objectID": "layers.html#gabor_",
    "href": "layers.html#gabor_",
    "title": "Functional layers",
    "section": "Gabor_",
    "text": "Gabor_\n\nsource\n\nGaborLayer_\n\n GaborLayer_ (n_scales:int, n_orientations:int,\n              kernel_size:Union[int,Sequence[int]], strides:int=1,\n              padding:str='SAME', feature_group_count:int=1,\n              kernel_init:Callable=&lt;function init&gt;,\n              bias_init:Callable=&lt;function zeros&gt;, use_bias:bool=False,\n              xmean:float=0.5, ymean:float=0.5, fs:float=1,\n              normalize_prob:bool=True, normalize_energy:bool=False,\n              zero_mean:bool=False, parent:Union[Type[flax.linen.module.Mo\n              dule],Type[flax.core.scope.Scope],Type[flax.linen.module._Se\n              ntinel],NoneType]=&lt;flax.linen.module._Sentinel object at\n              0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nParametric Gabor layer with particular initialization.\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,1))\nmodel = GaborLayerLogSigma_(n_scales=4, n_orientations=10, kernel_size=64, fs=64, normalize_prob=True, normalize_energy=False)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    freq: (4,),\n    logsigmax2: (4,),\n    logsigmay2: (4,),\n    sigma_theta: (10,),\n    theta: (10,),\n})\n\n\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = GaborLayerGamma_(n_scales=4, n_orientations=10, kernel_size=64, fs=64, normalize_prob=True, normalize_energy=False)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    freq: (4,),\n    gammax: (4,),\n    gammay: (4,),\n    sigma_theta: (10,),\n    sigmax: (4,),\n    sigmay: (4,),\n    theta: (10,),\n})\n\n\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(28,28,3))\nmodel = GaborLayer_(n_scales=4, n_orientations=10, kernel_size=64, fs=64, normalize_prob=True, normalize_energy=False)\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\nprint(\"Parameter shapes: \", jax.tree_util.tree_map(lambda x: x.shape, params))\n\nParameter shapes:  FrozenDict({\n    freq: (4,),\n    sigma_theta: (10,),\n    sigmax: (4,),\n    sigmay: (4,),\n    theta: (10,),\n})\n\n\n\nparams\n\nFrozenDict({\n    freq: Array([24., 12.,  6.,  3.], dtype=float32),\n    sigmax: Array([0.01666667, 0.03333334, 0.06666667, 0.13333334], dtype=float32),\n    sigmay: Array([0.025     , 0.05      , 0.10000001, 0.20000002], dtype=float32),\n    theta: Array([0.        , 0.31415927, 0.62831855, 0.9424778 , 1.2566371 ,\n           1.5707964 , 1.8849556 , 2.1991148 , 2.5132742 , 2.8274336 ],      dtype=float32),\n    sigma_theta: Array([0.        , 0.31415927, 0.62831855, 0.9424778 , 1.2566371 ,\n           1.5707964 , 1.8849556 , 2.1991148 , 2.5132742 , 2.8274336 ],      dtype=float32),\n})\n\n\n\nx, y = model.generate_dominion()\nkernel = jax.vmap(GaborLayer_.gabor, in_axes=(None,None,None,None,0,0,0,None,None,None,None,None,None), out_axes=0)\n# kernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,0,None,None,None,None,None,None), out_axes=0)\nkernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,0,0,None,None,None,None), out_axes=0)\nkernel = jax.vmap(kernel, in_axes=(None,None,None,None,None,None,None,None,None,0,None,None,None), out_axes=0)(x, y, model.xmean, model.ymean, params[\"sigmax\"], params[\"sigmay\"], params[\"freq\"], params[\"theta\"], params[\"sigma_theta\"], model.phase, 1, model.normalize_prob, model.normalize_energy)\n# kernel = rearrange(kernel, \"phases rots fs_sigmas kx ky -&gt; kx ky (phases rots fs_sigmas)\")\n# kernel = repeat(kernel, \"kx ky c_out -&gt; kx ky c_in c_out\", c_in=3, c_out=kernel.shape[-1])\nkernel.shape\n\n(2, 10, 4, 64, 64)\n\n\n\nfig, axes = plt.subplots(kernel.shape[1],kernel.shape[2])\nfor i, axs in enumerate(axes):\n    for j, ax in enumerate(axs):\n        ax.imshow(kernel[0,i,j])\n        ax.axis(\"off\")\nplt.show()\n\nfig, axes = plt.subplots(kernel.shape[1],kernel.shape[2])\nfor i, axs in enumerate(axes):\n    for j, ax in enumerate(axs):\n        ax.imshow(kernel[1,i,j])\n        ax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nkernel_f = rearrange(kernel[1:2], \"phases rots fs_sigmas kx ky -&gt; (phases rots fs_sigmas) kx ky\")\nkernel_f.shape\n\n(40, 64, 64)\n\n\n\nkernel_f_fft = jnp.fft.fftn(kernel_f)\nkernel_f_fft = jnp.fft.fftshift(kernel_f_fft)\nkernel_f_fft_abs_sum = jnp.abs(kernel_f_fft).sum(axis=0)\nkernel_f_fft.shape, kernel_f_fft_abs_sum.shape\n\n((40, 64, 64), (64, 64))\n\n\n\nfig, axes = plt.subplots(kernel.shape[1],kernel.shape[2])\nfor i, axs in enumerate(axes):\n    for j, ax in enumerate(axs):\n        ax.imshow(jnp.abs(jnp.fft.fftshift(jnp.fft.fftn(kernel[0,i,j]))))\n        ax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\nfig, axes = plt.subplots(kernel.shape[1],kernel.shape[2])\nfor i, axs in enumerate(axes):\n    for j, ax in enumerate(axs):\n        ax.imshow(jnp.abs(jnp.fft.fftshift(jnp.fft.fftn(kernel[1,i,j]))))\n        ax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nplt.imshow(kernel_f_fft_abs_sum)\nplt.show()\n\n\n\n\n\n# #| eval: false\n# import matplotlib.pyplot as plt\n# kernel = model.return_kernel(params, c_in=3)\n# fig, axes = plt.subplots(1, 3)\n# # for k, sigmax, sigmay, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), params[\"sigmax\"], params[\"sigmay\"], axes):\n# for k, sigmax, sigmay, ax in zip(rearrange(kernel, \"kx ky cin cout -&gt; (cin cout) kx ky\"), jnp.exp(params[\"sigmax\"]), jnp.exp(params[\"sigmay\"]), axes):\n#     ax.imshow(k)\n#     ax.set_title(f\"{sigmax:.2f}, {sigmay:.2f}\")\n# plt.show()"
  },
  {
    "objectID": "layers.html#j-h",
    "href": "layers.html#j-h",
    "title": "Functional layers",
    "section": "J & H",
    "text": "J & H\n\nsource\n\nJamesonHurvich\n\n JamesonHurvich (parent:Union[Type[flax.linen.module.Module],Type[flax.cor\n                 e.scope.Scope],Type[flax.linen.module._Sentinel],NoneType\n                 ]=&lt;flax.linen.module._Sentinel object at 0x7f1c5f19dcd0&gt;,\n                 name:Optional[str]=None)\n\nJameson & Hurvich transformation from RGB to ATD.\n\nimport cv2\nloros = cv2.imread(\"loros.jpeg\")\nloros = cv2.cvtColor(loros, cv2.COLOR_BGR2RGB) / 255.0\nplt.imshow(loros)\nplt.show()\n\n\n\n\n\nT = JamesonHurvich()\npred = T.apply({\"params\":{\"\":[]}}, loros)\nfig, axes = plt.subplots(1,3, figsize=(20,4))\nfor i, ax in enumerate(axes):\n    ax.imshow(pred[:,:,i], cmap=\"gray\")\nplt.show()"
  },
  {
    "objectID": "layers.html#csf",
    "href": "layers.html#csf",
    "title": "Functional layers",
    "section": "CSF",
    "text": "CSF\n\nsource\n\nCSFFourier\n\n CSFFourier (fs:int=64, norm_energy:bool=True, parent:Union[Type[flax.line\n             n.module.Module],Type[flax.core.scope.Scope],Type[flax.linen.\n             module._Sentinel],NoneType]=&lt;flax.linen.module._Sentinel\n             object at 0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nCSF SSO.\n\nmodel = CSFFourier()\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,29,3))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\ndef forward(params, inputs):\n    return model.apply({\"params\": params}, inputs)\n\n\n_ = forward(params, x)\n_ = jax.jit(forward)(params, x)\n\n\nfrom functools import partial\n\n\nf = jax.jit(model.csf_sso, static_argnums=(1,2))\nf(fs=64, Nx=4, Ny=4, alpha=1., beta=1.)\nf(fs=64, Nx=6, Ny=6, alpha=1., beta=1.)\n\n(Array([[7.4208830e-04, 2.5290015e-01, 2.0652041e+00, 4.0784965e+00,\n         2.0652041e+00, 2.5290015e-01],\n        [2.5290015e-01, 5.6689572e-02, 4.7199564e+00, 1.7653423e+01,\n         4.7199564e+00, 5.6689572e-02],\n        [2.0652041e+00, 4.7199564e+00, 4.3300481e+00, 7.6411346e+01,\n         4.3300481e+00, 4.7199564e+00],\n        [4.0784965e+00, 1.7653423e+01, 7.6411346e+01, 5.3465088e+01,\n         7.6411346e+01, 1.7653423e+01],\n        [2.0652041e+00, 4.7199564e+00, 4.3300481e+00, 7.6411346e+01,\n         4.3300481e+00, 4.7199564e+00],\n        [2.5290015e-01, 5.6689572e-02, 4.7199564e+00, 1.7653423e+01,\n         4.7199564e+00, 5.6689572e-02]], dtype=float32),\n Array([[-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334]], dtype=float32),\n Array([[-32.      , -32.      , -32.      , -32.      , -32.      ,\n         -32.      ],\n        [-21.333334, -21.333334, -21.333334, -21.333334, -21.333334,\n         -21.333334],\n        [-10.666667, -10.666667, -10.666667, -10.666667, -10.666667,\n         -10.666667],\n        [  0.      ,   0.      ,   0.      ,   0.      ,   0.      ,\n           0.      ],\n        [ 10.666667,  10.666667,  10.666667,  10.666667,  10.666667,\n          10.666667],\n        [ 21.333334,  21.333334,  21.333334,  21.333334,  21.333334,\n          21.333334]], dtype=float32))\n\n\n\n@partial(jax.jit, static_argnums=(0,1))\ndef test(Nx, Ny):\n    return model.csf_sso(fs=64, Nx=Nx, Ny=Ny, alpha=1., beta=1.)\ntest(Nx=4, Ny=4)\ntest(Nx=6, Ny=6)\n\n(Array([[7.4208830e-04, 2.5290015e-01, 2.0652041e+00, 4.0784965e+00,\n         2.0652041e+00, 2.5290015e-01],\n        [2.5290015e-01, 5.6689572e-02, 4.7199564e+00, 1.7653423e+01,\n         4.7199564e+00, 5.6689572e-02],\n        [2.0652041e+00, 4.7199564e+00, 4.3300481e+00, 7.6411346e+01,\n         4.3300481e+00, 4.7199564e+00],\n        [4.0784965e+00, 1.7653423e+01, 7.6411346e+01, 5.3465088e+01,\n         7.6411346e+01, 1.7653423e+01],\n        [2.0652041e+00, 4.7199564e+00, 4.3300481e+00, 7.6411346e+01,\n         4.3300481e+00, 4.7199564e+00],\n        [2.5290015e-01, 5.6689572e-02, 4.7199564e+00, 1.7653423e+01,\n         4.7199564e+00, 5.6689572e-02]], dtype=float32),\n Array([[-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334]], dtype=float32),\n Array([[-32.      , -32.      , -32.      , -32.      , -32.      ,\n         -32.      ],\n        [-21.333334, -21.333334, -21.333334, -21.333334, -21.333334,\n         -21.333334],\n        [-10.666667, -10.666667, -10.666667, -10.666667, -10.666667,\n         -10.666667],\n        [  0.      ,   0.      ,   0.      ,   0.      ,   0.      ,\n           0.      ],\n        [ 10.666667,  10.666667,  10.666667,  10.666667,  10.666667,\n          10.666667],\n        [ 21.333334,  21.333334,  21.333334,  21.333334,  21.333334,\n          21.333334]], dtype=float32))\n\n\n\nf = jax.jit(model.csf_chrom, static_argnums=(1,2))\n# f = jax.jit(model.csf_chrom)\nf(fs=64, Nx=4, Ny=4, alpha_rg=1., alpha_yb=1., beta=1.)\nf(fs=64, Nx=6, Ny=6, alpha_rg=1., alpha_yb=1., beta=1.)\n\n(Array([[  0.1658539 ,   0.46152785,   0.940704  ,   1.220865  ,\n           0.940704  ,   0.46152785],\n        [  0.46152785,   1.6083002 ,   4.1651654 ,   6.0859256 ,\n           4.1651654 ,   1.6083002 ],\n        [  0.940704  ,   4.1651654 ,  15.595571  ,  30.33703   ,\n          15.595571  ,   4.1651654 ],\n        [  1.220865  ,   6.0859256 ,  30.33703   , 150.97502   ,\n          30.33703   ,   6.0859256 ],\n        [  0.940704  ,   4.1651654 ,  15.595571  ,  30.33703   ,\n          15.595571  ,   4.1651654 ],\n        [  0.46152785,   1.6083002 ,   4.1651654 ,   6.0859256 ,\n           4.1651654 ,   1.6083002 ]], dtype=float32),\n Array([[1.9348152e-03, 1.0028565e-02, 3.1509679e-02, 4.7914241e-02,\n         3.1509679e-02, 1.0028565e-02],\n        [1.0028565e-02, 7.4629605e-02, 3.4461936e-01, 6.3405633e-01,\n         3.4461936e-01, 7.4629605e-02],\n        [3.1509679e-02, 3.4461936e-01, 2.8785884e+00, 8.3903189e+00,\n         2.8785884e+00, 3.4461936e-01],\n        [4.7914241e-02, 6.3405633e-01, 8.3903189e+00, 1.1071500e+02,\n         8.3903189e+00, 6.3405633e-01],\n        [3.1509679e-02, 3.4461936e-01, 2.8785884e+00, 8.3903189e+00,\n         2.8785884e+00, 3.4461936e-01],\n        [1.0028565e-02, 7.4629605e-02, 3.4461936e-01, 6.3405633e-01,\n         3.4461936e-01, 7.4629605e-02]], dtype=float32),\n Array([[-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334]], dtype=float32),\n Array([[-32.      , -32.      , -32.      , -32.      , -32.      ,\n         -32.      ],\n        [-21.333334, -21.333334, -21.333334, -21.333334, -21.333334,\n         -21.333334],\n        [-10.666667, -10.666667, -10.666667, -10.666667, -10.666667,\n         -10.666667],\n        [  0.      ,   0.      ,   0.      ,   0.      ,   0.      ,\n           0.      ],\n        [ 10.666667,  10.666667,  10.666667,  10.666667,  10.666667,\n          10.666667],\n        [ 21.333334,  21.333334,  21.333334,  21.333334,  21.333334,\n          21.333334]], dtype=float32))\n\n\n\npartial(jax.jit, static_argnums=(0,1))\ndef test(Nx, Ny):\n    return model.csf_chrom(fs=64, Nx=Nx, Ny=Ny, alpha_rg=1., alpha_yb=1., beta=1)\ntest(Nx=4, Ny=4)\ntest(Nx=6, Ny=6)\n\n(Array([[  0.1658539 ,   0.46152785,   0.940704  ,   1.220865  ,\n           0.940704  ,   0.46152785],\n        [  0.46152785,   1.6083002 ,   4.1651654 ,   6.0859256 ,\n           4.1651654 ,   1.6083002 ],\n        [  0.940704  ,   4.1651654 ,  15.595571  ,  30.33703   ,\n          15.595571  ,   4.1651654 ],\n        [  1.220865  ,   6.0859256 ,  30.33703   , 150.97502   ,\n          30.33703   ,   6.0859256 ],\n        [  0.940704  ,   4.1651654 ,  15.595571  ,  30.33703   ,\n          15.595571  ,   4.1651654 ],\n        [  0.46152785,   1.6083002 ,   4.1651654 ,   6.0859256 ,\n           4.1651654 ,   1.6083002 ]], dtype=float32),\n Array([[1.9348152e-03, 1.0028565e-02, 3.1509679e-02, 4.7914241e-02,\n         3.1509679e-02, 1.0028565e-02],\n        [1.0028565e-02, 7.4629605e-02, 3.4461936e-01, 6.3405633e-01,\n         3.4461936e-01, 7.4629605e-02],\n        [3.1509679e-02, 3.4461936e-01, 2.8785884e+00, 8.3903189e+00,\n         2.8785884e+00, 3.4461936e-01],\n        [4.7914241e-02, 6.3405633e-01, 8.3903189e+00, 1.1071500e+02,\n         8.3903189e+00, 6.3405633e-01],\n        [3.1509679e-02, 3.4461936e-01, 2.8785884e+00, 8.3903189e+00,\n         2.8785884e+00, 3.4461936e-01],\n        [1.0028565e-02, 7.4629605e-02, 3.4461936e-01, 6.3405633e-01,\n         3.4461936e-01, 7.4629605e-02]], dtype=float32),\n Array([[-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334],\n        [-32.      , -21.333334, -10.666667,   0.      ,  10.666667,\n          21.333334]], dtype=float32),\n Array([[-32.      , -32.      , -32.      , -32.      , -32.      ,\n         -32.      ],\n        [-21.333334, -21.333334, -21.333334, -21.333334, -21.333334,\n         -21.333334],\n        [-10.666667, -10.666667, -10.666667, -10.666667, -10.666667,\n         -10.666667],\n        [  0.      ,   0.      ,   0.      ,   0.      ,   0.      ,\n           0.      ],\n        [ 10.666667,  10.666667,  10.666667,  10.666667,  10.666667,\n          10.666667],\n        [ 21.333334,  21.333334,  21.333334,  21.333334,  21.333334,\n          21.333334]], dtype=float32))\n\n\n\nT = JamesonHurvich()\nloros_atd = T.apply({\"params\":{\"\":[]}}, loros)\n\n\npred = model.apply({\"params\": params}, loros_atd[None,:])\npred.shape\n\n(1, 183, 275, 3)\n\n\n\nMng2xyz = jnp.array([[69.1661, 52.4902, 46.6052],\n                                  [39.0454, 115.8404, 16.3118],\n                                  [3.3467, 12.6700, 170.1090]])\nMxyz2atd = jnp.array([[0, 1, 0],\n                            [1, -1, 0],\n                            [0, 0.4, -0.4]])\nMxyz2ng = jnp.linalg.inv(Mng2xyz)\nMatd2xyz = jnp.linalg.inv(Mxyz2atd)\n\n\npred_rg = (pred @ Matd2xyz.T @ Mxyz2ng.T)**(1/2)\npred_rg.shape\n\n(1, 183, 275, 3)\n\n\n\nplt.imshow(pred_rg[0])\nplt.show()\n\n\n\n\n\nfig, axes = plt.subplots(1,3, figsize=(20,4))\nfor i in range(3):\n    axes[i].imshow(pred[0][:,:,i], cmap=\"gray\")\n    axes[i].axis(\"off\")\nplt.show()\n\n\n\n\n\ndef step(params):\n    pred = model.apply({\"params\": params}, loros_atd[None,:])\n    return jnp.mean((pred - loros_atd[None,:])**2)\n\n\n@jax.jit\ndef train_step(params, image, lr=3e-4):\n    def step(params):\n        pred = model.apply(params, image)\n        return jnp.mean((pred - image)**2)\n    \n    loss, grad = jax.value_and_grad(step)(params)\n    params = jax.tree_util.tree_map(lambda p,g: p - lr*g, params, grad)\n    return params, loss\n\n\nlr = 3e-4\nparams_ = {\"params\": params}\nfor it in range(N_ITERS:=3):\n    params_, loss = train_step(params_, loros_atd[None,:], lr=lr)\n    print(f\"It: {it} -&gt; {loss}\")\nparams_\n\nIt: 0 -&gt; 1214.27734375\nIt: 1 -&gt; 273.94989013671875\nIt: 2 -&gt; 497.3630676269531\n\n\n{'params': FrozenDict({\n     alpha_achrom: Array(1.1354797, dtype=float32),\n     alpha_chrom_rg: Array(1.316926, dtype=float32),\n     alpha_chrom_yb: Array(1.0976608, dtype=float32),\n     beta_achrom: Array(1.2981634, dtype=float32),\n     beta_chrom: Array(0.5935609, dtype=float32),\n     fm: Array(7.2472496, dtype=float32),\n     s: Array(1.8248081, dtype=float32),\n })}\n\n\n\nlr = 3e-4\nparams_ = params\nfor it in range(N_ITERS:=3):\n    loss, grad = jax.value_and_grad(step)(params_)\n    params_ = jax.tree_util.tree_map(lambda p,g: p - lr*g, params_, grad)\n    print(f\"It: {it} -&gt; {loss}\")\nparams_\n\nIt: 0 -&gt; 1214.27734375\nIt: 1 -&gt; 273.9499206542969\nIt: 2 -&gt; 497.36279296875\n\n\nFrozenDict({\n    alpha_achrom: Array(1.1354793, dtype=float32),\n    alpha_chrom_rg: Array(1.3169256, dtype=float32),\n    alpha_chrom_yb: Array(1.0976607, dtype=float32),\n    beta_achrom: Array(1.2981635, dtype=float32),\n    beta_chrom: Array(0.5935616, dtype=float32),\n    fm: Array(7.2472496, dtype=float32),\n    s: Array(1.8248082, dtype=float32),\n})\n\n\n\n## DN después de la CSF\n# b = 0.04 [0-1]\n# sigma = 0.02 (deg)\n\n\nfs = 64\nNx, Ny = loros[None,:].shape[1:3]\nalpha = 1.\nbeta = 1.\n# for beta in [10., 1., 0.8, 0.5, 0.01, 0.]:\nfor p in [params, params_]:\n    csf, _, _ = model.csf_sso(fs, Nx, Ny, p[\"alpha_achrom\"], p[\"beta_achrom\"], g=330.74, fm=p[\"fm\"], l=0.837, s=p[\"s\"], w=1.0, os=6.664)\n    E1 = jnp.sum(jnp.ones_like(csf)**2)#**(1/2)\n    E_CSF = jnp.sum(csf**2)#**(1/2)\n    csf = (csf/E_CSF)*E1\n    plt.plot(csf[csf.shape[0]//2], label=beta)\n# plt.legend()\nplt.ylim([0,0.2])\nplt.show()\n\n\n\n\n\nfs = 64\nNx, Ny = loros[None,:].shape[1:3]\nalpha = 1.\nbeta = 1.\nfor beta in [10., 1., 0.8, 0.5, 0.01, 0.]:\n    csf, _, _ = model.csf_sso(fs, Nx, Ny, alpha, beta, g=330.74, fm=7.28, l=0.837, s=1.809, w=1.0, os=6.664)\n    E1 = jnp.sum(jnp.ones_like(csf)**2)#**(1/2)\n    E_CSF = jnp.sum(csf**2)#**(1/2)\n    csf = (csf/E_CSF)*E1\n    plt.plot(csf[csf.shape[0]//2], label=beta)\nplt.legend()\nplt.ylim([0,0.2])\nplt.show()"
  },
  {
    "objectID": "layers.html#gdn",
    "href": "layers.html#gdn",
    "title": "Functional layers",
    "section": "GDN",
    "text": "GDN\n\nsource\n\nGDN\n\n GDN (kernel_size:Union[int,Sequence[int]], strides:int=1,\n      padding:str='SAME', apply_independently:bool=False,\n      kernel_init:Callable=&lt;function init&gt;, bias_init:Callable=&lt;function\n      ones&gt;, alpha:float=2.0, epsilon:float=0.5, eps:float=1e-06, parent:U\n      nion[Type[flax.linen.module.Module],Type[flax.core.scope.Scope],Type\n      [flax.linen.module._Sentinel],NoneType]=&lt;flax.linen.module._Sentinel\n      object at 0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nGeneralized Divisive Normalization."
  },
  {
    "objectID": "layers.html#gdn-star",
    "href": "layers.html#gdn-star",
    "title": "Functional layers",
    "section": "GDN star",
    "text": "GDN star\n\nsource\n\nGDNStar\n\n GDNStar (kernel_size:Sequence[int], apply_independently:bool=False,\n          inputs_star:Union[float,Sequence[float]]=1.0, alpha:float=2.0,\n          epsilon:float=0.5, kernel_init:Callable=&lt;function ones&gt;,\n          bias_init:Callable=&lt;function ones&gt;, parent:Union[Type[flax.linen\n          .module.Module],Type[flax.core.scope.Scope],Type[flax.linen.modu\n          le._Sentinel],NoneType]=&lt;flax.linen.module._Sentinel object at\n          0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nGDN variation that forces the output to be 1 when the input is x^*\n\nmodel = GDNStar(kernel_size=(1,1))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,28,1))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\nparams = unfreeze(params)\nparams[\"Conv_0\"][\"kernel\"] *= 0.\nparams[\"Conv_0\"][\"kernel\"] += 1.\nparams[\"Conv_0\"][\"bias\"] += 1.\nparams = freeze(params)\nparams\n\nFrozenDict({\n    Conv_0: {\n        bias: Array([1.], dtype=float32),\n        kernel: Array([[[[1.]]]], dtype=float32),\n    },\n})\n\n\n\ninputs = jnp.linspace(0, 5, num=28*28).reshape((1,28,28,1))\n\n\nfor inputs_star in [1, 2, 3, 4]:\n    model.inputs_star = inputs_star\n    pred = model.apply({\"params\": params}, inputs)\n    plt.scatter(inputs.ravel(), pred.ravel(), label=inputs_star)\n    plt.axvline(inputs_star)\n\nplt.axhline(1)\nplt.legend()\nplt.show()\n\n\n\n\n\nmodel.inputs_star = 1\nfor H in [1., 2., 3., 4., 5.]:\n    params = unfreeze(params)\n    params[\"Conv_0\"][\"kernel\"] *= 0.\n    params[\"Conv_0\"][\"kernel\"] += H\n    params = freeze(params)\n    pred = model.apply({\"params\": params}, inputs)\n    plt.scatter(inputs.ravel(), pred.ravel(), label=H)\nplt.axhline(1)\nplt.axvline(1)\nplt.legend()\nplt.xlim([0,2])\nplt.show()\n\n\n\n\nWe can even specify a different value of \\(x^*\\) for each input channel:\n\nmodel = GDNStar(kernel_size=(1,1), inputs_star=jnp.array([1.,2.,3.,4.]))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\ninputs = jnp.ones(shape=(1,28,28,4))\nvariables = model.init(key2, inputs)\nstate, params = variables.pop(\"params\")\npred = model.apply({\"params\": params}, inputs)\nassert len(jnp.unique(jnp.unique(jnp.unique(pred, 0), 0), 0)) == 4"
  },
  {
    "objectID": "layers.html#gdn-star-positives-and-negatives",
    "href": "layers.html#gdn-star-positives-and-negatives",
    "title": "Functional layers",
    "section": "GDN star (positives and negatives)",
    "text": "GDN star (positives and negatives)\n\nsource\n\nGDNStarSign\n\n GDNStarSign (kernel_size:Sequence[int], apply_independently:bool=False,\n              inputs_star:Union[float,Sequence[float]]=1.0,\n              alpha:float=2.0, epsilon:float=0.5, parent:Union[Type[flax.l\n              inen.module.Module],Type[flax.core.scope.Scope],Type[flax.li\n              nen.module._Sentinel],NoneType]=&lt;flax.linen.module._Sentinel\n              object at 0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\nGDN variation that forces the output to be 1 when the input is x^*\n\nmodel = GDNStarSign(kernel_size=(1,1))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,28,1))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\nparams = unfreeze(params)\nparams[\"Conv_0\"][\"kernel\"] *= 0.\nparams[\"Conv_0\"][\"kernel\"] += 1.\nparams[\"Conv_0\"][\"bias\"] += 1.\nparams = freeze(params)\nparams\n\nFrozenDict({\n    Conv_0: {\n        bias: Array([1.], dtype=float32),\n        kernel: Array([[[[1.]]]], dtype=float32),\n    },\n})\n\n\n\ninputs = jnp.linspace(-5, 5, num=28*28).reshape((1,28,28,1))\n\n\nfor inputs_star in [1, 2, 3, 4]:\n    model.inputs_star = inputs_star\n    pred = model.apply({\"params\": params}, inputs)\n    plt.scatter(inputs.ravel(), pred.ravel(), label=inputs_star)\n    plt.axvline(inputs_star)\n    plt.axvline(-inputs_star)\n\nplt.axhline(1)\nplt.axhline(-1)\nplt.legend()\nplt.show()\n\n\n\n\n\nmodel.inputs_star = 1\nfor H in [1., 2., 3., 4., 5.]:\n    params = unfreeze(params)\n    params[\"Conv_0\"][\"kernel\"] *= 0.\n    params[\"Conv_0\"][\"kernel\"] += H\n    params = freeze(params)\n    pred = model.apply({\"params\": params}, inputs)\n    plt.scatter(inputs.ravel(), pred.ravel(), label=H)\nplt.axhline(1)\nplt.axvline(1)\nplt.axhline(-1)\nplt.axvline(-1)\nplt.legend()\n# plt.xlim([0,2])\nplt.show()"
  },
  {
    "objectID": "layers.html#gdn-displacement",
    "href": "layers.html#gdn-displacement",
    "title": "Functional layers",
    "section": "GDN (Displacement)",
    "text": "GDN (Displacement)\n\ninputs = jnp.linspace(-10, 10, num=28*28).reshape((1,28,28,1))\n\n\nsource\n\nGDNDisplacement\n\n GDNDisplacement (kernel_size:Sequence[int],\n                  apply_independently:bool=False,\n                  inputs_star:Union[float,Sequence[float]]=1.0,\n                  alpha:float=2.0, epsilon:float=0.5, parent:Union[Type[fl\n                  ax.linen.module.Module],Type[flax.core.scope.Scope],Type\n                  [flax.linen.module._Sentinel],NoneType]=&lt;flax.linen.modu\n                  le._Sentinel object at 0x7f1c5f19dcd0&gt;,\n                  name:Optional[str]=None)\n\nGDN variation that forces the output to be 1 when the input is x^*\n\nmodel = GDNDisplacement(kernel_size=(1,1))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,28,1))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\nparams = unfreeze(params)\nparams[\"Conv_0\"][\"kernel\"] *= 0.\nparams[\"Conv_0\"][\"kernel\"] += 1.\nparams[\"Conv_0\"][\"bias\"] += 1.\nparams = freeze(params)\nparams\n\nFrozenDict({\n    Conv_0: {\n        bias: Array([1.], dtype=float32),\n        kernel: Array([[[[1.]]]], dtype=float32),\n    },\n})\n\n\n\nfor inputs_mean in [0, 1, 2, 3, 4]:\n    inputs_ = jnp.linspace(-5, 5, num=28*28).reshape((1,28,28,1)) + inputs_mean\n    pred = model.apply({\"params\": params}, inputs_)\n    plt.scatter(inputs_.ravel(), pred.ravel(), label=inputs_mean, s=4)\n    plt.axvline(inputs_mean)\n\nplt.axhline(1)\nplt.axhline(-1)\nplt.legend()\nplt.show()\n\n\n\n\n\nsource\n\n\nGDNStarDisplacement\n\n GDNStarDisplacement (kernel_size:Sequence[int],\n                      apply_independently:bool=False,\n                      inputs_star:Union[float,Sequence[float]]=1.0,\n                      alpha:float=2.0, epsilon:float=0.5, parent:Union[Typ\n                      e[flax.linen.module.Module],Type[flax.core.scope.Sco\n                      pe],Type[flax.linen.module._Sentinel],NoneType]=&lt;fla\n                      x.linen.module._Sentinel object at 0x7f1c5f19dcd0&gt;,\n                      name:Optional[str]=None)\n\nGDN variation that forces the output to be 1 when the input is x^*\n\nmodel = GDNStarDisplacement(kernel_size=(1,1))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,28,1))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\nparams = unfreeze(params)\nparams[\"Conv_0\"][\"kernel\"] *= 0.\nparams[\"Conv_0\"][\"kernel\"] += 1.\nparams[\"Conv_0\"][\"bias\"] += 1.\nparams = freeze(params)\nparams\n\nFrozenDict({\n    Conv_0: {\n        bias: Array([1.], dtype=float32),\n        kernel: Array([[[[1.]]]], dtype=float32),\n    },\n})\n\n\n\nfor inputs_mean in [0, 1, 2, 3, 4]:\n    inputs_ = jnp.linspace(-5, 5, num=28*28).reshape((1,28,28,1)) + inputs_mean\n    pred = model.apply({\"params\": params}, inputs_)\n    plt.scatter(inputs_.ravel(), pred.ravel(), label=inputs_mean, s=4)\nplt.axvline(1)\n\nplt.axhline(1)\nplt.axhline(-1)\nplt.legend()\n# plt.ylim([-1,1])\nplt.show()\n\n\n\n\n\ninputs_mean = 0.\nfor inputs_star in [1, 2, 3, 4]:\n    for inputs_mean in [0, 1, 2, 3, 4]:\n        model.inputs_star = inputs_star\n        inputs_ = jnp.linspace(-5, 5, num=28*28).reshape((1,28,28,1)) + inputs_mean\n        pred = model.apply({\"params\": params}, inputs_)\n        plt.scatter(inputs_.ravel(), pred.ravel(), label=f\"x*={inputs_star} / x_mean={inputs_mean}\", s=4)\nplt.axvline(0)\n\nplt.axhline(1)\nplt.axhline(-1)\n# plt.legend()\n# plt.ylim([-1,1])\nplt.show()"
  },
  {
    "objectID": "layers.html#gdn-batch-norm-like",
    "href": "layers.html#gdn-batch-norm-like",
    "title": "Functional layers",
    "section": "GDN Batch-Norm-Like",
    "text": "GDN Batch-Norm-Like\n\nsource\n\nGDNStarRunning\n\n GDNStarRunning (kernel_size:Sequence[int],\n                 apply_independently:bool=False, alpha:float=2.0,\n                 epsilon:float=0.5, bias_init:Callable=&lt;function ones&gt;, pa\n                 rent:Union[Type[flax.linen.module.Module],Type[flax.core.\n                 scope.Scope],Type[flax.linen.module._Sentinel],NoneType]=\n                 &lt;flax.linen.module._Sentinel object at 0x7f1c5f19dcd0&gt;,\n                 name:Optional[str]=None)\n\nGDN variation where x^* is obtained as a running mean of the previously obtained values.\n\nmodel = GDNStarRunning(kernel_size=(1,1))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,28,1))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\nparams = unfreeze(params)\nparams[\"Conv_0\"][\"kernel\"] *= 0.\nparams[\"Conv_0\"][\"kernel\"] += 1.\nparams[\"Conv_0\"][\"bias\"] += 1.\nparams = freeze(params)\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, train=False, mutable=list(state.keys()))\nassert updated_state == state\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, train=True, mutable=list(state.keys()))\nassert updated_state != state\n\n\nupdated_state = state\nfor inputs_mean in [0, 1, 2, 3, 4, 5, 2]:\n    inputs_ = jnp.linspace(0, 5, num=28*28).reshape((1,28,28,1))# + inputs_mean\n    pred, updated_state = model.apply({\"params\": params, **updated_state}, inputs_, train=True, mutable=list(state.keys()))\n    print(updated_state[\"batch_stats\"][\"inputs_star\"])\n    plt.scatter(inputs_.ravel(), pred.ravel(), label=inputs_mean, s=4)\nplt.axvline(1)\n\nplt.axhline(1)\nplt.legend()\n# plt.ylim([-1,1])\nplt.show()\n\n[2.875]\n[3.8125]\n[4.28125]\n[4.515625]\n[4.6328125]\n[4.6914062]\n[4.720703]"
  },
  {
    "objectID": "layers.html#gdn-batch-norm-like-displacement",
    "href": "layers.html#gdn-batch-norm-like-displacement",
    "title": "Functional layers",
    "section": "GDN Batch-Norm-Like (Displacement)",
    "text": "GDN Batch-Norm-Like (Displacement)\n\nsource\n\nGDNStarDisplacementRunning\n\n GDNStarDisplacementRunning (kernel_size:Sequence[int],\n                             apply_independently:bool=False,\n                             alpha:float=2.0, epsilon:float=0.5, parent:Un\n                             ion[Type[flax.linen.module.Module],Type[flax.\n                             core.scope.Scope],Type[flax.linen.module._Sen\n                             tinel],NoneType]=&lt;flax.linen.module._Sentinel\n                             object at 0x7f1c5f19dcd0&gt;,\n                             name:Optional[str]=None)\n\nGDN variation where x^* is obtained as a running mean of the previously obtained values.\n\nmodel = GDNStarDisplacementRunning(kernel_size=(1,1))\nkey1, key2 = random.split(random.PRNGKey(0), 2)\nx = random.normal(key1, shape=(1,28,28,1))\nvariables = model.init(key2, x)\nstate, params = variables.pop(\"params\")\n\n\nparams = unfreeze(params)\nparams[\"Conv_0\"][\"kernel\"] *= 0.\nparams[\"Conv_0\"][\"kernel\"] += 1.\nparams[\"Conv_0\"][\"bias\"] += 1.\nparams = freeze(params)\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, train=False, mutable=list(state.keys()))\nassert updated_state == state\n\n\noutputs, updated_state = model.apply({\"params\": params, **state}, x, train=True, mutable=list(state.keys()))\nassert updated_state != state\n\n\nupdated_state = state\nfor inputs_mean in [0, 1, 2, 3, 4, 5, 2]:\n    inputs_ = jnp.linspace(-5, 5, num=28*28).reshape((1,28,28,1)) + inputs_mean\n    pred, updated_state = model.apply({\"params\": params, **updated_state}, inputs_, train=True, mutable=list(state.keys()))\n    print(updated_state[\"batch_stats\"][\"inputs_star\"])\n    plt.scatter(inputs_.ravel(), pred.ravel(), label=inputs_mean, s=4)\nplt.axvline(1)\n\nplt.axhline(1)\nplt.axhline(-1)\nplt.legend()\n# plt.ylim([-1,1])\nplt.show()\n\n[2.8777137]\n[4.188857]\n[5.3444285]\n[6.4222145]\n[7.4611073]\n[8.480554]\n[7.490277]"
  },
  {
    "objectID": "layers.html#gaussian-frequency-interaction",
    "href": "layers.html#gaussian-frequency-interaction",
    "title": "Functional layers",
    "section": "Gaussian Frequency Interaction",
    "text": "Gaussian Frequency Interaction\nIn some cases it may make sense to model specifically the interaction between frequencies so that closer frequencies interact more than further away frequencies, so we can design a GDN that takes this into account by introducing a Gaussian into it.\nMay be necessary to combine the interaction and the Gabor in the same layer so that the Gaussian can receive the Gabor frequencies.\n\nsource\n\nFreqGaussian\n\n FreqGaussian (use_bias:bool=False, strides:int=1, padding:str='SAME',\n               bias_init:Callable=&lt;function zeros&gt;, parent:Union[Type[flax\n               .linen.module.Module],Type[flax.core.scope.Scope],Type[flax\n               .linen.module._Sentinel],NoneType]=&lt;flax.linen.module._Sent\n               inel object at 0x7f1c5f19dcd0&gt;, name:Optional[str]=None)\n\n(1D) Gaussian interaction between frequencies.\n\nfmean = jnp.array([2., 8., 4.])\nmodel = FreqGaussian()\nvariables = model.init(random.PRNGKey(42), jnp.ones(shape=(1,28,28,3)), fmean=fmean)\nstate, params = variables.pop(\"params\")\n\n\nparams\n\nFrozenDict({\n    sigma: Array([0.8, 3.2, 1.6], dtype=float32),\n})\n\n\n\nsigma = jnp.array([5, 0.8, 1.2])\nkernel = jax.vmap(model.gaussian, in_axes=(None,0,0,None))(fmean, fmean, params[\"sigma\"], 1)\nprint(kernel.shape)\nplt.matshow(kernel)\nplt.xticks(ticks=range(len(params[\"sigma\"])), labels=params[\"sigma\"])\nplt.yticks(ticks=range(len(params[\"sigma\"])), labels=params[\"sigma\"])\nplt.show()\n\n(3, 3)\n\n\n\n\n\nTo test it in combination with the GaborLayer_ we can define a simple module combining them both. This is needed because we need to be able to get the freq from the Gabors and then pass it to the FreqGaussian layer.\n\nclass Model(nn.Module):\n    \"\"\"Dummy model to test the combination of the `GaborLayer_` and the `FreqGaussian` layer.\"\"\"\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs, fmean = GaborLayer_(n_scales=4, n_orientations=10, kernel_size=64, fs=64, normalize_prob=True, normalize_energy=False)(inputs, **kwargs, return_freq=True)\n        outputs = FreqGaussian()(outputs, fmean=fmean)\n        return outputs\n\n\nmodel = Model()\nvariables = model.init(random.PRNGKey(42), jnp.ones(shape=(1,28,28,1)))\nstate, params = variables.pop(\"params\")"
  },
  {
    "objectID": "layers.html#orient-gaussian",
    "href": "layers.html#orient-gaussian",
    "title": "Functional layers",
    "section": "Orient Gaussian",
    "text": "Orient Gaussian\nAlmost like the FreqGaussian but taking into account that degrees are in a circular domain.\n\nassert wrapTo180(0) == 0\nassert wrapTo180(181) == -179\nassert wrapTo180(-190) == 170\n\n\ntheta = 380\nangle = process_angles(jnp.array([0., 45.]), jnp.array([362., -45.]))#*jnp.pi/180.\nangle\n\nArray([ 2., 90.], dtype=float32)\n\n\n\nsource\n\nOrientGaussian\n\n OrientGaussian (use_bias:bool=False, strides:int=1, padding:str='SAME',\n                 bias_init:Callable=&lt;function zeros&gt;, parent:Union[Type[fl\n                 ax.linen.module.Module],Type[flax.core.scope.Scope],Type[\n                 flax.linen.module._Sentinel],NoneType]=&lt;flax.linen.module\n                 ._Sentinel object at 0x7f1c5f19dcd0&gt;,\n                 name:Optional[str]=None)\n\n(1D) Gaussian interaction between orientations.\n\ntheta_mean = jnp.array([45, 90, 359])\nmodel = OrientGaussian()\nvariables = model.init(random.PRNGKey(42), jnp.ones(shape=(1,28,28,3)), theta_mean=theta_mean)\nstate, params = variables.pop(\"params\")\n\n\nt = jnp.linspace(-1., 361., num=100)\nk = jax.vmap(model.gaussian, in_axes=(None,0,None,None))(t, theta_mean, 5., 1)\nfor kk, tm in zip(k, theta_mean): plt.plot(t, kk, label=tm)\nfor tm in [0., 90., 180., 270., 360.]: plt.axvline(tm)\nplt.legend()\nplt.show()\n\n\n\n\nWhen we are considering a 0 deg filter, we want it to have the maximum relation with both 0 deg and 180 deg. By visualizing the previous graph, we see that we are obtaining what we wanted.\n\nparams\n\nFrozenDict({\n    sigma: Array([30., 30., 30.], dtype=float32),\n})\n\n\n\nkernel = jax.vmap(model.gaussian, in_axes=(None,0,0,None))(theta_mean, theta_mean, params[\"sigma\"], 1)\nprint(kernel.shape)\nplt.matshow(kernel)\nplt.xticks(ticks=range(len(params[\"sigma\"])), labels=theta_mean)\nplt.yticks(ticks=range(len(params[\"sigma\"])), labels=theta_mean)\nplt.show()\n\n(3, 3)\n\n\n\n\n\nTo test it in combination with the GaborLayer_ we can define a simple module combining them both. This is needed because we need to be able to get the theta from the Gabors and then pass it to the OrientGaussian layer.\n\nA very important difference with respect to the FreqGaussian case is that the Gabor filters in GaborLayer_ need to be reordered so that we can apply them in the same way.\n\n\nclass Model(nn.Module):\n    \"\"\"Dummy model to test the combination of the `GaborLayer_` and the `OrientGaussian` layer.\"\"\"\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        b, h, w, c = inputs.shape\n        outputs, theta_mean = GaborLayer_(n_scales=4, n_orientations=10, kernel_size=64, fs=64, normalize_prob=True, normalize_energy=False)(inputs, **kwargs, return_theta=True)\n        ## Reshape so that the orientations are the innermost dimmension\n        outputs = rearrange(outputs, \"b h w (phase theta f) -&gt; b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=4, theta=10)\n        outputs = OrientGaussian()(outputs, theta_mean=theta_mean)\n        ## Recover original disposition\n        outputs = rearrange(outputs, \"b h w (phase f theta) -&gt; b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=4, theta=10)\n        return outputs\n\n\nmodel = Model()\nvariables = model.init(random.PRNGKey(42), jnp.ones(shape=(1,28,28,1)))\nstate, params = variables.pop(\"params\")\n\n\n\nPutting it all together\nWe can even combine both layers in a separable-interaction-way.\n\nclass Model(nn.Module):\n    \"\"\"Dummy model to test the combination of the `GaborLayer_` and the `OrientGaussian` layer.\"\"\"\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        b, h, w, c = inputs.shape\n        outputs, fmean, theta_mean = GaborLayer_(n_scales=4, n_orientations=10, kernel_size=64, fs=64, normalize_prob=True, normalize_energy=False)(inputs, return_freq=True, return_theta=True, **kwargs)\n        outputs = FreqGaussian()(outputs, fmean=fmean)\n        ## Reshape so that the orientations are the innermost dimmension\n        outputs = rearrange(outputs, \"b h w (phase theta f) -&gt; b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=4, theta=10)\n        outputs = OrientGaussian()(outputs, theta_mean=theta_mean)\n        ## Recover original disposition\n        outputs = rearrange(outputs, \"b h w (phase f theta) -&gt; b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=4, theta=10)\n        return outputs\n\n\nmodel = Model()\nvariables = model.init(random.PRNGKey(42), jnp.ones(shape=(1,28,28,1)))\nstate, params = variables.pop(\"params\")"
  }
]